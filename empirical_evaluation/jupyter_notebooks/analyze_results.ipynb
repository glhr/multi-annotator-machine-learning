{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c074a60a078dedc",
   "metadata": {},
   "source": [
    "# `crowd-hpo`: Results\n",
    "\n",
    "This notebook allows us to query tabular results via [`mlflow`](https://mlflow.org/). As a prerequisite, the experiments have to be performed in a first step. If this is the case, we can load the results for the different tables in the accompanied article. Update\n",
    "- `MLRUNS_PATH` to the path, where the results are stored according to your config file [`experiment.yaml`](../conf/experiment.yaml), \n",
    "- `FIGURES_PATH` to the path, where any figures are to be saved,\n",
    "- `CACHE_PATH` to the path, where any intermediate results loaded via mlfow are to be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from typing import Optional\n",
    "from hydra.utils import to_absolute_path\n",
    "from mlflow import set_tracking_uri, get_experiment_by_name, search_runs\n",
    "from scipy.stats import percentileofscore\n",
    "\n",
    "# Adjust global constants as needed.\n",
    "MLRUNS_PATH = \"/mnt/work/mherde/maml/crowd_hpo/\"\n",
    "FIGURES_PATH = \"/mnt/home/mherde/projects/github/multi-annotator-machine-learning/empirical_evaluation/figures/\"\n",
    "CACHE_PATH = \"/mnt/home/mherde/projects/github/multi-annotator-machine-learning/empirical_evaluation/python_scripts/results_hpo_backup\"\n",
    "\n",
    "# Filter meaningless warnings.\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define global color setings.\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"dejavuserif\"\n",
    "colors = [\n",
    "    (0.0, (0, 128/255, 128/255)),   \n",
    "    (0.5, (1.0, 1.0, 1.0)),           \n",
    "    (1.0, (127/255, 0, 127/255)), \n",
    "]\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"DivergingTealPurple\", colors)\n",
    "\n",
    "# Increase maximum number of displayed rows.\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "\n",
    "# Flags for output formatting\n",
    "SHOW_LATEX = True\n",
    "SHOW_INDIVIDUAL_SCORES = False\n",
    "\n",
    "# Define global constants for the visualization of results.\n",
    "UPDATE_COLUMNS = {\n",
    "    \"params.data.optimizer.gt_params.lr\": \"gt_lr\",\n",
    "    \"params.data.optimizer.gt_params.weight_decay\": \"gt_wd\",\n",
    "    \"params.data.optimizer.ap_params.lr\": \"ap_lr\",\n",
    "    \"params.data.optimizer.ap_params.weight_decay\": \"ap_wd\",\n",
    "    \"params.data.train_batch_size\": \"bs\",\n",
    "    \"params.data.max_epochs\": \"me\",\n",
    "    \"params.data.lr_scheduler.params.T_max\": \"T_max\",\n",
    "    \"params.architecture.params.dropout_rate\": \"dr\",\n",
    "    \"params.classifier.params.lmbda\": \"lmbda\",\n",
    "    \"params.classifier.params.eta\": \"eta\",\n",
    "    \"params.classifier.params.alpha\": \"alpha\",\n",
    "    \"params.classifier.params.beta\": \"beta\",\n",
    "    \"params.classifier.embed_size\": \"dim\",\n",
    "    \"params.classifier.params.epsilon\": \"epsilon\",\n",
    "}\n",
    "INV_UPDATE_COLUMNS = {v: k for k, v in UPDATE_COLUMNS.items()}\n",
    "APPROACHES = [\n",
    "    \"ground-truth\",\n",
    "    \"majority-vote\",\n",
    "    \"dawid-skene\",\n",
    "    \"crowd-layer\",\n",
    "    \"trace-reg\",\n",
    "    \"conal\",\n",
    "    \"union-net-a\",\n",
    "    \"union-net-b\",\n",
    "    \"geo-reg-w\",\n",
    "    \"geo-reg-f\",\n",
    "    \"madl\",\n",
    "    \"crowd-ar\",\n",
    "    \"annot-mix\",\n",
    "    \"coin-net\",\n",
    "]\n",
    "DATASETS = []\n",
    "for v in [\"worst-1\", \"worst-2\", \"worst-var\", \"rand-1\", \"rand-2\", \"rand-var\", \"full\"]:\n",
    "    DATASETS.extend(\n",
    "        [\n",
    "        f\"music_genres_{v}\",\n",
    "        f\"label_me_{v}\",\n",
    "        f\"dopanim_{v}\",\n",
    "        f\"reuters_{v}\",\n",
    "        f\"spc_{v}\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "LOSS_FUNC = \"zero_one_loss\"\n",
    "\n",
    "def process_df(df, clf_col='clf', agg_col='agg'):\n",
    "    \"\"\"\n",
    "    Process a DataFrame by combining classifier and aggregator columns, cleaning specific substrings, \n",
    "    dropping the aggregator column, and reordering columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing at least the columns specified by `clf_col` and `agg_col`.\n",
    "    clf_col : str, optional\n",
    "        The name of the classifier column. Default is 'clf'.\n",
    "    agg_col : str, optional\n",
    "        The name of the aggregator column. Default is 'agg'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The processed DataFrame with a combined column (named after `clf_col`) placed at the beginning, \n",
    "        with the original `agg_col` column removed and specific substrings ('aggregate_' and '_None') replaced \n",
    "        with empty strings.\n",
    "    \"\"\"\n",
    "    # Reorder columns so that the combined column comes first\n",
    "    cols = [clf_col] + [col for col in df.columns if col != clf_col]\n",
    "    return df[cols]\n",
    "\n",
    "def evaluate(\n",
    "    mlruns_path: str,\n",
    "    experiment_name: str,\n",
    "    update_columns: dict = None,\n",
    "    perf_type: str = \"gt\",\n",
    "    version: str = \"test\",\n",
    "    metric: str = \"acc\",\n",
    "    epoch: str = \"best\",\n",
    "    loss_func: str = \"zero_one_loss\",\n",
    "    print_number_of_runs: bool = True,\n",
    "    cache_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Queries the evaluation results via mlflow.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mlruns_path : str\n",
    "        Path to the results saved via mlflow.\n",
    "    experiment_name : str\n",
    "        Name of the mlflow experiment.\n",
    "    update_columns : dict, default=None\n",
    "        Optional dictionary of columns to be included in the output table.\n",
    "        If None, the global UPDATE_COLUMNS is used.\n",
    "    perf_type : str, default=\"gt\"\n",
    "        'gt' for ground-truth estimates or 'ap' for annotator performance estimates.\n",
    "    version : str, default=\"test\"\n",
    "        One of 'train', 'valid', or 'test' for different data subsets.\n",
    "    metric : str, default=\"acc\"\n",
    "        Performance metric: 'acc', 'brier_score', or 'tce'.\n",
    "    epoch : str, default=\"best\"\n",
    "        Either 'last' or 'best' epoch.\n",
    "    print_number_of_runs : bool, default=False\n",
    "        Whether to print the count of runs per classifier.\n",
    "    cache_path : Optional[str], default=None\n",
    "        If provided, results are cached/read from the given path.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    runs: pd.DataFrame or None\n",
    "        Table of results or None if no experiment/runs found.\n",
    "    \"\"\"\n",
    "    runs = None\n",
    "    \n",
    "    # Use the provided update_columns or fall back to the default global constant.\n",
    "    if update_columns is None:\n",
    "        update_columns = UPDATE_COLUMNS.copy()\n",
    "    \n",
    "    # Attempt to load from cache if available.\n",
    "    if cache_path:\n",
    "        cache_file = os.path.join(cache_path, f\"{experiment_name}.csv\")\n",
    "        if os.path.isfile(cache_file):\n",
    "            runs = pd.read_csv(cache_file)\n",
    "            if \"hyperparameter_search\" in experiment_name:\n",
    "                print(cache_file)\n",
    "                runs = runs.append(pd.read_csv(cache_file.replace(\"hyperparameter_search\", \"default\")))\n",
    "    \n",
    "    if runs is None:\n",
    "        # Set mlflow tracking URI.\n",
    "        abs_mlruns_path = to_absolute_path(mlruns_path)\n",
    "        set_tracking_uri(uri=f\"file://{abs_mlruns_path}\")\n",
    "\n",
    "        exp = get_experiment_by_name(experiment_name)\n",
    "        if exp is None:\n",
    "            return None\n",
    "\n",
    "        query = \"status = 'FINISHED'\"\n",
    "    \n",
    "        runs = search_runs(experiment_ids=exp.experiment_id, filter_string=query, output_format=\"pandas\")\n",
    "        if runs.empty:\n",
    "            return None\n",
    "        if cache_path:\n",
    "            try:\n",
    "                runs.to_csv(cache_file, index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Unable to cache results due to error: {e}\")\n",
    "                \n",
    "    # Base column renaming.\n",
    "    rename_columns = {\n",
    "        \"params.data.class_definition._target_\": \"data\",\n",
    "        \"params.classifier.name\": \"clf\",\n",
    "        \"params.classifier.aggregation_method\": \"agg\",\n",
    "    }\n",
    "    # Update renaming mapping if provided columns exist in runs.\n",
    "    for col, new_name in update_columns.items():\n",
    "        if col in runs.columns:\n",
    "            rename_columns[col] = new_name\n",
    "\n",
    "    # Build aggregation dictionary.\n",
    "    agg_dict = {\n",
    "        f\"metrics.{perf_type}_true_{loss_func}_{epoch}_test\": [\"mean\", \"std\"],\n",
    "        \"params.seed\": [\"sum\"],\n",
    "    }\n",
    "    bayes_columns = []\n",
    "    for col in runs.columns:\n",
    "        if col.startswith(\"metrics.\") and col.endswith(\"_valid\") and \"bayes\" not in col:\n",
    "            agg_dict[col] = [\"mean\", \"std\"]\n",
    "\n",
    "    # Drop duplicates using selected columns.\n",
    "    # Note: Ensure that 'params.data.class_definition.realistic_split' exists in runs.\n",
    "    dup_keys = list(rename_columns.keys()) + [\"params.seed\", \"params.data.class_definition.realistic_split\"]\n",
    "    runs = runs.drop_duplicates(subset=dup_keys)#.fillna(np.inf)\n",
    "    \n",
    "    if print_number_of_runs:\n",
    "        # Actually print the value counts.\n",
    "        print(runs[\"params.classifier.name\"].value_counts().sum())\n",
    "\n",
    "    runs = runs.sort_values(by=\"params.seed\")\n",
    "    runs = runs.groupby(list(rename_columns.keys()), as_index=False, dropna=False).agg(agg_dict, skipna=False)\n",
    "\n",
    "    # Flatten multi-index columns if necessary.\n",
    "    if isinstance(runs.columns, pd.MultiIndex):\n",
    "        runs.columns = [\"\".join(map(str, col)).strip() for col in runs.columns.values]\n",
    "\n",
    "    # Round 'metrics' columns for accuracy if they exist.\n",
    "    for col in runs.columns:\n",
    "        if col.startswith(\"metrics\") and \"zero_one_loss\" in col:\n",
    "            runs[col] = np.round(runs[col] * 100, 3)\n",
    "\n",
    "    # Add an extra column mapping.\n",
    "    rename_columns[\"params.seedsum\"] = \"n_runs\"\n",
    "\n",
    "    # Build a combined renaming mapping for any remaining columns.\n",
    "    additional_rename = {\n",
    "        col: col.replace(\"mean\", \"\")\n",
    "        .replace(\"max\", \"\")\n",
    "        .replace(\"std\", \"_std\")\n",
    "        .replace(\"-\", \"_\")\n",
    "        .replace(\"metrics.\", \"\")\n",
    "        for col in runs.columns\n",
    "        if col not in rename_columns\n",
    "    }\n",
    "    # Merge both dictionaries. (Note: The \"|\" operator requires Python 3.9+)\n",
    "    full_rename = rename_columns | additional_rename\n",
    "    runs = runs.rename(columns=full_rename)\n",
    "    return runs\n",
    "    \n",
    "def plot_rankings(ranking_df, file_path, approaches_key=\"clf\", mean_axis=\"columns\"):\n",
    "    # Extract metric labels from the DataFrame columns (skip the first column)\n",
    "    metrics = [c.replace('_', '-').replace('-target', '') for c in ranking_df.columns[1:]]\n",
    "    # Extract the list of approaches (from the first column of the DataFrame)\n",
    "    approaches = ranking_df[approaches_key].values\n",
    "\n",
    "    # Compute the ranking matrix from numeric columns\n",
    "    ranking_matrix = ranking_df.iloc[:, 1:].values.round(2)\n",
    "    \n",
    "    if mean_axis == \"columns\":\n",
    "        # Compute the mean and standard deviation for each column (exclude the first row)\n",
    "        means = ranking_df.iloc[1:, 1:].mean(axis=0).values.round(2)\n",
    "        stds  = ranking_df.iloc[1:, 1:].std(axis=0).values.round(2)\n",
    "        # Create a spacer row (filled with NaN) to visually separate the aggregated row\n",
    "        spacer = np.full((1, means.shape[0]), np.nan)\n",
    "        # Append the spacer and then the mean row to the ranking matrix\n",
    "        ranking_matrix = np.vstack([ranking_matrix, spacer, means])\n",
    "        # Append empty label for spacer and 'Mean' for the aggregated row to the list of approaches\n",
    "        approaches = np.append(approaches, ['', 'Mean'])\n",
    "        # The index of the \"Mean\" row in the augmented matrix\n",
    "        mean_row_index = ranking_matrix.shape[0] - 1\n",
    "\n",
    "    elif mean_axis == \"rows\":\n",
    "        # Compute the mean and std for each row (over all numeric columns)\n",
    "        means = ranking_df.iloc[:, 1:].mean(axis=1).values.round(2)\n",
    "        stds  = ranking_df.iloc[:, 1:].std(axis=1).values.round(2)\n",
    "        # Create a spacer column (filled with NaN)\n",
    "        spacer = np.full((means.shape[0], 1), np.nan)\n",
    "        # Append the spacer and then the mean column to the ranking matrix\n",
    "        ranking_matrix = np.column_stack([ranking_matrix, spacer, means])\n",
    "        # Append empty label and 'Mean' to metrics\n",
    "        metrics.append('')\n",
    "        metrics.append('Mean')\n",
    "        # The index of the \"Mean\" column in the augmented matrix\n",
    "        mean_col_index = ranking_matrix.shape[1] - 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mean_axis must be either 'rows' or 'columns'.\")\n",
    "\n",
    "    # Plotting\n",
    "    # We use imshow on the transpose so that the x-axis displays the aggregated row labels (approaches)\n",
    "    plt.figure(figsize=(0.8*ranking_matrix.shape[0], 0.6*ranking_matrix.shape[1]))\n",
    "    plt.imshow(ranking_matrix.T, cmap=custom_cmap, vmin=0, vmax=1, alpha=1, aspect=\"auto\")\n",
    "\n",
    "    n_rows, n_cols = ranking_matrix.shape\n",
    "\n",
    "    # Annotate each cell in the matrix\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            val = ranking_matrix[i, j]\n",
    "            # Skip annotation for spacer cells (which are NaN)\n",
    "            if np.isnan(val):\n",
    "                continue\n",
    "            # For the aggregated mean cells, add the standard deviation info\n",
    "            #if mean_axis == \"columns\" and i == mean_row_index:\n",
    "            #    # The j-th column's std deviation from the computed stds array\n",
    "            #    txt = f\"{val:.2f}\\n±{stds[j]:.2f}\"\n",
    "            #elif mean_axis == \"rows\" and j == mean_col_index:\n",
    "            #    # For each row, annotate the aggregated column with mean and std dev\n",
    "            #    txt = f\"{val:.2f}\\n±{stds[i]:.2f}\"\n",
    "            #else:\n",
    "            txt = f\"{val:.2f}\"\n",
    "            # Because we used ranking_matrix.T in imshow, the text positions are swapped.\n",
    "            plt.text(i, j, txt, ha=\"center\", va=\"center\", color='black', fontsize=16)\n",
    "\n",
    "    # Adjust tick labels.\n",
    "    # With imshow(ranking_matrix.T), the x-axis corresponds to the original rows (approaches),\n",
    "    # and the y-axis to the original columns (metrics).\n",
    "    plt.xticks(np.arange(n_rows), approaches, rotation=90)\n",
    "    plt.tick_params(top=True, labeltop=True, bottom=False, labelbottom=False)\n",
    "    plt.yticks(np.arange(n_cols), metrics)\n",
    "    #plt.colorbar()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6126324e5beefe8",
   "metadata": {},
   "source": [
    "## Hyperparameter Study\n",
    "\n",
    "Print the across all dataset variants, *learning from crowds* (LFC) approaches, and *hyperparameter* (HPS) selection criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c61e2d50a6b082",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict = {}\n",
    "approach_rankings = None\n",
    "metric_rankings = None\n",
    "for ds in DATASETS:\n",
    "    print(ds)\n",
    "    results_df_dict = {}\n",
    "    for experiment_type in [\"hyperparameter_search\", \"default\",  f\"default_data\"]:\n",
    "        print(experiment_type)\n",
    "        # Load results per dataset.\n",
    "        exp_name = f\"{experiment_type}_{ds}\"\n",
    "        runs_df = evaluate(\n",
    "            mlruns_path=os.path.join(MLRUNS_PATH, exp_name),\n",
    "            experiment_name=exp_name,\n",
    "            update_columns=UPDATE_COLUMNS,\n",
    "            perf_type=\"class\",\n",
    "            version=\"valid\",\n",
    "            epoch=\"full\",\n",
    "            loss_func=LOSS_FUNC,\n",
    "            cache_path=CACHE_PATH,\n",
    "        )\n",
    "        if runs_df is None:\n",
    "            break\n",
    "\n",
    "        # Preprocess columns.\n",
    "        df = runs_df.drop(columns=[\"data\"])\n",
    "        df['clf'] = df['clf'].astype(str) + '-' + df['agg'].astype(str)\n",
    "        df.drop(columns=['agg'], inplace=True)\n",
    "\n",
    "        # Identify '_valid' columns & rank them within each group\n",
    "        target_col = f\"class_true_{LOSS_FUNC}_full_test\"\n",
    "        group_cols = [\"clf\"]\n",
    "        valid_cols = [c for c in df.columns if c.endswith(\"valid\")]\n",
    "\n",
    "        if SHOW_INDIVIDUAL_SCORES:\n",
    "\n",
    "            # Identify non-valid and group columns\n",
    "            other_cols = [col for col in df.columns if not col.endswith(\"valid\") and col not in group_cols]\n",
    "\n",
    "            # Function that, for each group, returns rows corresponding \n",
    "            # to the max values for each \"valid\" column.\n",
    "            def get_max_rows(grp):\n",
    "                d = {}\n",
    "                for col in valid_cols:\n",
    "                    idx = grp[col].idxmin()\n",
    "                    if not np.isnan(idx):\n",
    "                        d[col] = grp.loc[idx, other_cols]\n",
    "                return pd.DataFrame(d).T\n",
    "\n",
    "            # Apply to each group\n",
    "            df_max = df.groupby(group_cols).apply(get_max_rows)\n",
    "\n",
    "            # df_max now has a multi-index with the group keys + the valid column name\n",
    "            # If you want a flat DataFrame, reset the index and rename columns\n",
    "            df_max = df_max.reset_index()\n",
    "            df_max.columns = group_cols + [\"valid_col\"] + other_cols\n",
    "            df_max = df_max.sort_values(by=[\"clf\", f\"class_true_{LOSS_FUNC}_full_test\"])\n",
    "            df_max[\"rank\"] = df_max.groupby(\"clf\")[f\"class_true_{LOSS_FUNC}_full_test\"].rank(\n",
    "                ascending=True, pct=True, method=\"average\"\n",
    "            )\n",
    "            df_max = df_max[(df_max[\"clf\"] == \"aggregate-ground-truth\") & (df_max[\"valid_col\"] == f\"class_true_{LOSS_FUNC}_cv_valid\")]\n",
    "            df_max = df_max.rename(columns=INV_UPDATE_COLUMNS)\n",
    "            drop_col = [c for c in df_max.columns if not \"params\" in c]\n",
    "            df_max = df_max.drop(columns=drop_col)\n",
    "            df_max = df_max.dropna(axis=1)\n",
    "            #print(df_max.to_markdown(tablefmt=\"github\"))\n",
    "            df_max = df_max.to_dict(\"list\")\n",
    "            df_max = {k[7:]: v for k, v in df_max.items() if \"alpha\" not in k}\n",
    "            print(df_max)\n",
    "            print()\n",
    "            continue\n",
    "\n",
    "\n",
    "        for col in valid_cols:\n",
    "            df[col + \"_rank\"] = df.groupby(group_cols)[col].rank(method=\"min\")\n",
    "\n",
    "        # Define subsets & compute mean rank per subset\n",
    "        subsets = {\n",
    "            # Lower and upper baseline scores.\n",
    "            \"TRUE\": [f\"class_true_{LOSS_FUNC}_cv_valid\"],\n",
    "            \n",
    "            \"AEU\": [f\"class_mv_unif_{LOSS_FUNC}_cv_valid\"],\n",
    "            \"AEC\": [f\"class_mv_unif_weighted_{LOSS_FUNC}_cv_valid\"],\n",
    "            \"ALU\": [f\"class_mv_log_odds_1_{LOSS_FUNC}_cv_valid\"],\n",
    "            \"ALC\": [f\"class_mv_log_odds_1_weighted_{LOSS_FUNC}_cv_valid\"],\n",
    "            \n",
    "            \"CXU\": [f\"annot_unif_{LOSS_FUNC}_cv_valid\"],\n",
    "            \"CEC\": [f\"annot_unif_weighted_{LOSS_FUNC}_cv_valid\"],\n",
    "            \"CLC\": [f\"annot_log_odds_1_weighted_{LOSS_FUNC}_cv_valid\"],\n",
    " \n",
    "            \"ENS\": [\n",
    "                #f\"class_mv_unif_{LOSS_FUNC}_cv_valid\",\n",
    "                #f\"class_mv_unif_weighted_{LOSS_FUNC}_cv_valid\",\n",
    "                f\"class_mv_log_odds_1_{LOSS_FUNC}_cv_valid\",\n",
    "                #f\"class_mv_log_odds_1_weighted_{LOSS_FUNC}_cv_valid\",\n",
    "                \n",
    "                #f\"annot_unif_{LOSS_FUNC}_cv_valid\",\n",
    "                #f\"annot_unif_weighted_{LOSS_FUNC}_cv_valid\",\n",
    "                #f\"annot_log_odds_1_weighted_{LOSS_FUNC}_cv_valid\",\n",
    "            ],\n",
    "        }\n",
    "        for subset_name, subset_cols in subsets.items():\n",
    "            rank_cols = [c + \"_rank\" for c in subset_cols]\n",
    "            try:\n",
    "                df[f\"{subset_name}_mean_rank\"] = df[rank_cols].mean(axis=1, skipna=True)\n",
    "            except:\n",
    "                df[f\"{subset_name}_mean_rank\"] = 100.0\n",
    "            #if len(subset_cols) > 1:\n",
    "            #    df[f\"{subset_name}_mean_rank\"] = StandardScaler().fit_transform(df[subset_cols].values).mean(axis=1)\n",
    "            if len(subset_cols) > 1:\n",
    "                try:\n",
    "                    #for clf in np.unqiue(df[\"clf\"].values):\n",
    "                    #    from scipy.stats import kendalltau\n",
    "                    #    df_madl = df[df[\"clf\"].values == f\"{clf}-None\"]\n",
    "                    #    tau = np.zeros((len(rank_cols), len(rank_cols)))\n",
    "                    #    for i, c_i in enumerate(rank_cols):\n",
    "                    #        for j, c_j in enumerate(rank_cols):\n",
    "                    #            tau[i, j] = kendalltau(df_madl[c_i], df_madl[c_j])[0]\n",
    "                    #    ones_vector = np.ones_like(rank_cols, dtype=float)\n",
    "                    #    tau_inv = np.linalg.pinv(tau)\n",
    "                    #    w =  (tau_inv @ ones_vector) / (ones_vector @ tau_inv @ ones_vector)\n",
    "                    #    df.loc[df[\"clf\"].values == f\"{clf}-None\", f\"{subset_name}_mean_rank\"] = np.sum(df_madl[rank_cols].values * w, axis=1)\n",
    "                    df[f\"{subset_name}_mean_rank\"] -= 0.001 * df[rank_cols].std(axis=1, skipna=True)\n",
    "                except:\n",
    "                    pass\n",
    "            #mean_cols = [c for c in subset_cols]\n",
    "            #df[f\"{subset_name}_mean_rank\"] = df[mean_cols].mean(axis=1, skipna=False)\n",
    "            #if len(subset_cols) > 1:\n",
    "            #    df[f\"{subset_name}_mean_rank\"] += df[mean_cols].std(axis=1, skipna=False)\n",
    "            # Weighted average (row-wise):\n",
    "            # weighted_mean = sum(w_i * x_i) / sum(w_i)\n",
    "            #weight_cols = [c + \"_std\" for c in subset_cols]\n",
    "            #ranks = df[rank_cols].values\n",
    "            #weights = 1/df[weight_cols]\n",
    "            if len(subset_cols) > 100:\n",
    "                if \"-1\" in ds or \"image\" in ds:\n",
    "                    try:\n",
    "                        df[f\"{subset_name}_mean_rank\"] = df[rank_cols[2:]].mean(axis=1, skipna=False)\n",
    "                    except:\n",
    "                        df[f\"{subset_name}_mean_rank\"] = 100.0\n",
    "                        \n",
    "            if subset_name == \"ENSN\":\n",
    "                print(\"weights:\")\n",
    "                print(len(subset_cols))\n",
    "                try:\n",
    "                    ranks = df[rank_cols].values\n",
    "                    for approach, w in w_dict.items():\n",
    "                        #weights = w#np.array([0.22, 0.09, 0.32, 0.06, 0.31])\n",
    "                        # build the boolean mask\n",
    "                        app = f\"aggregate-{approach}\" if approach == \"dawid-skene\" else f\"{approach}-None\"\n",
    "\n",
    "                        # compute the weighted mean for just those rows\n",
    "                        # • if `ranks` is a NumPy array:\n",
    "                        weights = w[\"_\".join(str(x) for x in ds.split(\"_\")[:-1])]\n",
    "                        print(weights)\n",
    "                        #weights = w[ds]\n",
    "                        weighted = (df[rank_cols].values[mask] * weights).sum(axis=1)\n",
    "\n",
    "                        # • if `ranks` is itself a DataFrame, you can do:\n",
    "                        # weighted = ranks.loc[mask].mul(w, axis=1).sum(axis=1) / w.sum()\n",
    "\n",
    "                        # assign it back with .loc\n",
    "                        df.loc[mask, f\"{subset_name}_mean_rank\"] = weighted\n",
    "                except:\n",
    "                    df[f\"{subset_name}_mean_rank\"] = 100\n",
    "                \n",
    "        # Start with a DataFrame of unique group columns.\n",
    "        results_df_dict[experiment_type] = {\n",
    "            \"mean\": df.drop_duplicates(group_cols)[group_cols].sort_values(group_cols).reset_index(drop=True)\n",
    "        }\n",
    "        results_df_dict[experiment_type][\"std\"] = results_df_dict[experiment_type][\"mean\"].copy()\n",
    "\n",
    "        # Loop over subsets to find best row -> then attach the 'target'.        \n",
    "        for subset_name in subsets:\n",
    "            mean_rank_col = f\"{subset_name}_mean_rank\"\n",
    "            best_idx = df.groupby(group_cols)[mean_rank_col].idxmin().dropna()\n",
    "\n",
    "            # Extract columns for merging, rename 'target' -> subset-specific\n",
    "            best_for_subset = df.loc[best_idx, group_cols + [target_col]].copy()\n",
    "            best_for_subset.rename(columns={target_col: f\"{subset_name}_target\"}, inplace=True)\n",
    "            results_df_dict[experiment_type][\"mean\"] = results_df_dict[experiment_type][\"mean\"].merge(best_for_subset, on=group_cols, how=\"left\")\n",
    "\n",
    "            best_for_subset = df.loc[best_idx, group_cols + [target_col + \"_std\"]].copy()\n",
    "            best_for_subset.rename(columns={target_col + \"_std\": f\"{subset_name}_target\"}, inplace=True)\n",
    "            results_df_dict[experiment_type][\"std\"] = results_df_dict[experiment_type][\"std\"].merge(best_for_subset, on=group_cols, how=\"left\")\n",
    "\n",
    "            \n",
    "        # Reformat resulting data frames.\n",
    "        for k in results_df_dict[experiment_type].keys():\n",
    "            results_df_dict[experiment_type][k].columns = [f\"{c.replace('_', '-').replace('-target', '')}\" for c in results_df_dict[experiment_type][k].columns]\n",
    "            results_df_dict[experiment_type][k]['clf'] = results_df_dict[experiment_type][k]['clf'].replace({\"aggregate-\": \"\", \"-None\": \"\", \"_\": \"-\"}, regex=True)\n",
    "            results_df_dict[experiment_type][k]['sort_key'] = results_df_dict[experiment_type][k]['clf'].apply(lambda x: APPROACHES.index(x))\n",
    "            results_df_dict[experiment_type][k] = results_df_dict[experiment_type][k].sort_values('sort_key').drop(columns='sort_key')\n",
    "            results_df_dict[experiment_type][k] = results_df_dict[experiment_type][k].set_index([\"clf\"])\n",
    "    if SHOW_INDIVIDUAL_SCORES:\n",
    "        continue\n",
    "    perf_cols = [\"DEF\", \"DEF-DATA\"] + list(subsets.keys())\n",
    "    perf_cols[0] = \"TRUE\"\n",
    "    perf_cols[2] = \"DEF\"\n",
    "    results_df_dict[\"hyperparameter_search\"][\"mean\"][\"DEF\"] = results_df_dict[\"default\"][\"mean\"][\"TRUE\"].values\n",
    "    results_df_dict[\"hyperparameter_search\"][\"std\"][\"DEF\"] = results_df_dict[\"default\"][\"std\"][\"TRUE\"].values\n",
    "    results_df_dict[\"hyperparameter_search\"][\"mean\"][\"DEF-DATA\"] = results_df_dict[f\"default_data\"][\"mean\"][\"TRUE\"].values\n",
    "    results_df_dict[\"hyperparameter_search\"][\"std\"][\"DEF-DATA\"] = results_df_dict[f\"default_data\"][\"std\"][\"TRUE\"].values\n",
    "    results_df_dict = {\"mean\": results_df_dict[\"hyperparameter_search\"][\"mean\"].reindex(perf_cols, axis=1), \"std\": results_df_dict[\"hyperparameter_search\"][\"std\"].reindex(perf_cols, axis=1)}\n",
    "    for stat in [\"mean\", \"std\"]:\n",
    "        results_df_dict[stat].loc[\"ground-truth\"][\"DEF-DATA\"] = results_df_dict[stat].loc[\"ground-truth\"][\"TRUE\"]\n",
    "    df_dict[ds] = results_df_dict\n",
    "    \n",
    "    #results_df_dict[\"mean\"][\"regression\"] = 0\n",
    "    #results_df_dict[\"std\"][\"regression\"] = 0\n",
    "    #for test_approach, test_approach_dict in test_losses.items():\n",
    "    #    results_df_dict[\"mean\"].loc[test_approach.replace(\"_\", \"-\"), \"regression\"] = test_approach_dict[ds]\n",
    "    #perf_cols.append(\"regression\")\n",
    "    \n",
    "    # Print results as markdown.\n",
    "    df_markdown = results_df_dict[\"mean\"].applymap(lambda x: f\"{x:.2f}\" if isinstance(x, (float, int)) else x) + \" ± \" + results_df_dict[\"std\"].applymap(lambda x: f\"{x:.2f}\" if isinstance(x, (float, int)) else x)\n",
    "    print(df_markdown.to_markdown(tablefmt=\"github\", floatfmt=\".4f\"))\n",
    "\n",
    "    if SHOW_LATEX:\n",
    "        df_latex = results_df_dict[\"mean\"].applymap(lambda x: f\"${x:.2f}\" if isinstance(x, (float, int)) else x) + \"_{\\pm \" + results_df_dict[\"std\"].applymap(lambda x: f\"{x:.2f}\" + \"}$\" if isinstance(x, (float, int)) else x)\n",
    "        df_latex.index = df_latex.index.map(lambda x: f\"\\\\texttt{{{x}}}\")\n",
    "        df_latex = df_latex.replace(\"$nan_{\\pm nan}$\", \"--\")\n",
    "        #print(df_latex.style.to_latex())\n",
    "\n",
    "        # Get the underlying DataFrames for means and standard deviations\n",
    "        mean_df = results_df_dict[\"mean\"]\n",
    "        std_df = results_df_dict[\"std\"]\n",
    "\n",
    "        # Compute the minimum value for each column (all columns)\n",
    "        col_min = mean_df[1:].min()\n",
    "\n",
    "        # Compute the minimum for each row only considering columns starting from the third column\n",
    "        row_min = mean_df.iloc[:, 1:].min(axis=1)\n",
    "\n",
    "        # Create a copy to store our LaTeX formatted strings\n",
    "        df_latex = mean_df.copy()\n",
    "\n",
    "        # Iterate over every cell to apply the formatting\n",
    "        for i in mean_df.index:\n",
    "            for j in mean_df.columns:\n",
    "                m_val = mean_df.loc[i, j]\n",
    "                s_val = std_df.loc[i, j]\n",
    "                if isinstance(m_val, (int, float)):\n",
    "                    # Format the mean value with two decimals\n",
    "                    formatted_mean = f\"{m_val:.2f}\"\n",
    "                    # Bold if it is the minimum in its column\n",
    "                    is_bold = m_val == col_min[j]\n",
    "                    # Underline only if this column is not one of the first two\n",
    "                    col_index = mean_df.columns.get_loc(j)\n",
    "                    is_underline = (col_index >= 1) and (m_val == row_min[i]) and i != \"ground-truth\"\n",
    "\n",
    "                    # Apply nested formatting if both conditions hold\n",
    "                    if is_bold and is_underline:\n",
    "                        formatted_mean = f\"\\\\underline{{\\\\textBF{{{formatted_mean}}}}}\"\n",
    "                    elif is_bold:\n",
    "                        formatted_mean = f\"\\\\textBF{{{formatted_mean}}}\"\n",
    "                    elif is_underline:\n",
    "                        formatted_mean = f\"\\\\underline{{{formatted_mean}}}\"\n",
    "\n",
    "                    # Format the standard deviation value\n",
    "                    if isinstance(s_val, (int, float)):\n",
    "                        formatted_std = f\"{s_val:.2f}\" if s_val < 10 else f\"{s_val:.1f}\"\n",
    "                    else:\n",
    "                        formatted_std = s_val\n",
    "                    # Combine into one LaTeX math mode string\n",
    "                    df_latex.loc[i, j] = f\"${formatted_mean}_{{\\\\pm {formatted_std}}}$\"\n",
    "                else:\n",
    "                    df_latex.loc[i, j] = m_val\n",
    "\n",
    "        # Format the index using typewriter font\n",
    "        approaches_replace = {\n",
    "            \"ground-truth\": \"gt\",\n",
    "            \"majority-vote\": \"mv\",\n",
    "            \"dawid-skene\": \"ds\",\n",
    "            \"crowd-layer\": \"cl\",\n",
    "            \"trace-reg\": \"trace\",\n",
    "            \"conal\": \"conal\",\n",
    "            \"union-net-a\": \"union-a\",\n",
    "            \"union-net-b\": \"union-b\",\n",
    "            \"geo-reg-w\": \"geo-w\",\n",
    "            \"geo-reg-f\": \"geo-f\",\n",
    "            \"madl\": \"madl\",\n",
    "            \"crowd-ar\": \"crowd-ar\",\n",
    "            \"annot-mix\": \"annot-mix\",\n",
    "            \"coin-net\": \"coin\",\n",
    "            \n",
    "        }\n",
    "        df_latex.index = df_latex.index.map(\n",
    "            lambda x: f\"\\\\texttt{{{approaches_replace[x]}}}\"\n",
    "        )\n",
    "\n",
    "        # Replace any problematic nan strings with a placeholder\n",
    "        df_latex = df_latex.replace(\"$nan_{\\\\pm nan}$\", \"N/A\")\n",
    "\n",
    "        # Print the LaTeX output from the styled DataFrame\n",
    "        print(df_latex.style.to_latex())\n",
    "        \n",
    "    # For each performance metric, compute the rank.\n",
    "    approach_rankings = {metric: {} for metric in perf_cols} if approach_rankings is None else approach_rankings\n",
    "    for metric in perf_cols:\n",
    "        results_df_dict[\"mean\"][f'{metric}_rank'] = results_df_dict[\"mean\"][metric].rank(method='average', pct=False)-1#results_df_dict[\"mean\"][metric].apply(lambda x: (results_df_dict[\"mean\"][metric] < x).sum()) / len(results_df_dict[\"mean\"][metric])#\n",
    "        results_df_dict[\"mean\"][f'{metric}_rank'] /= results_df_dict[\"mean\"][metric].rank(method='average', pct=False).max()\n",
    "        \n",
    "    # For each metric, rank the approaches.\n",
    "    for key, row in results_df_dict[\"mean\"].iterrows():\n",
    "        for metric in perf_cols:\n",
    "            if key not in approach_rankings[metric]:\n",
    "                approach_rankings[metric][key] = []\n",
    "            approach_rankings[metric][key].append(row[f'{metric}_rank'])\n",
    "\n",
    "    # For each approach, rank its performance metrics.\n",
    "    metric_rankings = {} if metric_rankings is None else metric_rankings\n",
    "    grouped = results_df_dict[\"mean\"].groupby(group_cols)[perf_cols].mean().reset_index()\n",
    "    grouped['sort_key'] = grouped['clf'].apply(lambda x: APPROACHES.index(x))\n",
    "    grouped = grouped.sort_values('sort_key').drop(columns='sort_key')\n",
    "    for approach, row in grouped.iterrows():\n",
    "        approach = row[\"clf\"]\n",
    "        perf_values = row[perf_cols]\n",
    "        metric_ranks = perf_values.rank(method='average', pct=False)-1#perf_values.apply(lambda x: (perf_values < x).sum()) / len(perf_values)#\n",
    "        metric_ranks /= perf_values.rank(method='average', pct=False).max()\n",
    "        if approach not in metric_rankings:\n",
    "            metric_rankings[approach] = {metric: [] for metric in perf_cols}\n",
    "        for metric in perf_cols:\n",
    "            metric_rankings[approach][metric].append(metric_ranks[metric])\n",
    "                \n",
    "# Plot ranking of approaches per metric.\n",
    "final_rankings = [\n",
    "    {\n",
    "        'clf': key,\n",
    "        **{metric: np.mean(approach_rankings[metric][key]) for metric in perf_cols}\n",
    "    }\n",
    "    for key in approach_rankings[perf_cols[0]].keys() if key != \"ground-truth\"\n",
    "]\n",
    "final_rank_df = pd.DataFrame(final_rankings)\n",
    "print(\"Average approach ranking (from approach_rankings)\")\n",
    "plot_rankings(final_rank_df, mean_axis=\"rows\", file_path=f\"ranking_metrics_{LOSS_FUNC}.pdf\")\n",
    "\n",
    "# Plot ranking of metrics per approach.\n",
    "final_data = [\n",
    "    {\n",
    "        'clf': key,\n",
    "        **{metric: np.mean(metric_dict[metric]) for metric in perf_cols}\n",
    "    }\n",
    "    for key, metric_dict in metric_rankings.items() if key != \"ground-truth\"\n",
    "]\n",
    "final_rank_df = pd.DataFrame(final_data)\n",
    "print(\"Average approach ranking (from metric_rankings)\")\n",
    "plot_rankings(final_rank_df, file_path=f\"ranking_approaches_{LOSS_FUNC}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8cd4a4-eace-4682-ae03-74cfe5a46dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulate performance data for 12 approaches.\n",
    "np.random.seed(0)\n",
    "approaches = APPROACHES[1:]\n",
    "\n",
    "# reuters\n",
    "def_performance = np.array([24.32, 19.88, 14.90, 16.61, 17.07, 23.36, 15.79, 20.16, 15.10, 14.99, 16.11, 27.09, 20.50])\n",
    "opt_performance = np.array([16.71, 11.64, 10.52, 11.50, 11.53, 15.53, 12.13, 09.45, 12.11, 10.22, 11.78, 10.33, 10.11])\n",
    "\n",
    "\n",
    "# Compute rankings (1=best) based on performance (higher is better).\n",
    "default_rank = np.argsort(def_performance).argsort() + 1\n",
    "optimized_rank = np.argsort(opt_performance).argsort() + 1\n",
    "\n",
    "# Sort approaches by default performance for clearer visualization.\n",
    "order = np.argsort(def_performance)\n",
    "approaches_sorted = np.array(approaches)[order]\n",
    "default_sorted = def_performance[order]\n",
    "optimized_sorted = opt_performance[order]\n",
    "default_rank_sorted = default_rank[order]\n",
    "optimized_rank_sorted = optimized_rank[order]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 4.25))\n",
    "\n",
    "# Plot dumbbell lines for performance gain.\n",
    "ax.hlines(y=approaches_sorted, xmin=default_sorted, xmax=optimized_sorted, color='gray', alpha=0.7, zorder=-3)\n",
    "\n",
    "# Annotate each marker with its ranking.\n",
    "for i, approach in enumerate(approaches_sorted):\n",
    "    # Annotate the default performance marker with its rank.\n",
    "    ax.text(default_sorted[i] + 0.2, approaches_sorted[i],\n",
    "            f'$\\\\#{default_rank_sorted[i]}$', color=(127/255, 0, 127/255), va='center', ha='left', fontsize=12)\n",
    "    # Annotate the optimized performance marker with its rank.\n",
    "    ax.text(optimized_sorted[i] - 0.2, approaches_sorted[i],\n",
    "            f'$\\\\#{optimized_rank_sorted[i]}$', color=(0, 127/255, 127/255), va='center', ha='right', fontsize=12)\n",
    "    \n",
    "# Plot markers for default and optimized performance.\n",
    "ax.scatter(default_sorted, approaches_sorted, color=(100/255, 0, 100/255), s=100, label='Default')\n",
    "ax.scatter(optimized_sorted, approaches_sorted, color=(0, 127/255, 127/255), s=100, label='Optimized')\n",
    "\n",
    "ax.set_xlabel('Zero-one Loss [%]')\n",
    "ax.set_xticks(np.arange(8, 30, 1))\n",
    "ax.set_xticklabels(np.arange(8, 30, 1), fontsize=12)\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"graphical_abstract.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5be0c-caed-4bfc-9f49-b7bc224a4779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames to hold win counts and total comparisons for each pair (i,j)\n",
    "win_counts = None\n",
    "total_counts = None\n",
    "\n",
    "ordered_criteria = list(subsets.keys())\n",
    "ordered_criteria.insert(1, \"DEF-DATA\")\n",
    "ordered_criteria.insert(2, \"DEF\")\n",
    "\n",
    "# Loop over each dataset and each variant (row)\n",
    "for ds_name, df in df_dict.items():\n",
    "    df = df[\"mean\"][ordered_criteria][1:]\n",
    "    if win_counts is None or total_counts is None:\n",
    "        # Initialize DataFrames to hold win counts and total comparisons for each pair (i,j)\n",
    "        win_counts = pd.DataFrame(0, index=df.columns, columns=df.columns, dtype=float)\n",
    "        total_counts = pd.DataFrame(0, index=df.columns, columns=df.columns, dtype=float)\n",
    "    for idx, row in df.iterrows():\n",
    "        #if idx not in [\"madl\"]:\n",
    "        #    continue\n",
    "        for i in df.columns:\n",
    "            for j in df.columns:\n",
    "                # Only compare if both values are not NaN\n",
    "                if pd.notnull(row[i]) and pd.notnull(row[j]):\n",
    "                    total_counts.loc[i, j] += 1\n",
    "                    if row[i] < row[j]:\n",
    "                        win_counts.loc[i, j] += 1\n",
    "                    #elif row[i] == row[j]:\n",
    "                    #    win_counts.loc[i, j] += 0.5\n",
    "\n",
    "# Compute winning percentage matrix (expressed as a percentage)\n",
    "winning_percentage = win_counts / total_counts * 100\n",
    "\n",
    "winning_diffs = ((winning_percentage.values.T - winning_percentage.values) > 0).astype(float)\n",
    "np.fill_diagonal(winning_diffs, np.nan)\n",
    "\n",
    "\n",
    "\n",
    "#winning_percentage[\"mean\"] = winning_percentage.values.mean(axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "np.fill_diagonal(winning_percentage.values, np.nan)\n",
    "fig, ax = plt.subplots(figsize=(15, 5.25))\n",
    "ax.pcolormesh(winning_diffs, cmap=custom_cmap, vmin=0, vmax=1, alpha=0.3, edgecolors=\"white\", linewidth=2, shading=\"flat\")\n",
    "ax.set_aspect(\"auto\")\n",
    "for i in range(winning_percentage.shape[0]):\n",
    "    for j in range(winning_percentage.shape[1]):\n",
    "        formatted_text = r'${:.2f}$'.format(winning_percentage.iloc[i, j]).replace(\"$nan$\", \"N/A\")\n",
    "        plt.text(j+0.5, i+0.56, formatted_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=18)#, bbox=dict(facecolor='white', edgecolor='none', pad=0))\n",
    "ax.set_xticks(np.arange(len(winning_percentage))+0.5, winning_percentage.index.values, fontsize=15)\n",
    "ax.set_yticks(np.arange(len(winning_percentage))+0.5, winning_percentage.index.values, fontsize=15)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"criteria_winning_matrix_{LOSS_FUNC}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2ef9d-2eba-46ad-b3b8-a1251548c852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize DataFrames to hold win counts and total comparisons for each pair (i,j)\n",
    "win_counts = None\n",
    "total_counts = None\n",
    "\n",
    "# Loop over each dataset and each variant (row)\n",
    "for ds_name, df in df_dict.items():\n",
    "    #if \"dopanim\" not in ds_name:\n",
    "    #    continue\n",
    "    #df_tmp = df[\"mean\"]\n",
    "    df = df[\"mean\"][[\"ENS\"]][1:]\n",
    "    #df.loc[\"annot-mix\"] = df_tmp[\"ENS\"].loc[\"annot-mix\"]\n",
    "    if win_counts is None or total_counts is None:\n",
    "        # Initialize DataFrames to hold win counts and total comparisons for each pair (i,j)\n",
    "        win_counts = pd.DataFrame(0, index=df.index, columns=df.index, dtype=float)\n",
    "        total_counts = pd.DataFrame(0, index=df.index, columns=df.index, dtype=float)\n",
    "    for i, row_i in df.iterrows():\n",
    "        for j, row_j in df.iterrows():\n",
    "            if pd.notnull(row_i.values[0]) and pd.notnull(row_j.values[0]):\n",
    "                total_counts.loc[i, j] += 1\n",
    "                if row_i.values[0] < row_j.values[0]:\n",
    "                    win_counts.loc[i, j] += 1\n",
    "                    #elif row[i] == row[j]:\n",
    "                    #    win_counts.loc[i, j] += 0.5\n",
    "\n",
    "# Compute winning percentage matrix (expressed as a percentage)\n",
    "winning_percentage = win_counts / total_counts * 100\n",
    "\n",
    "winning_diffs = ((winning_percentage.values.T - winning_percentage.values) > 0).astype(float)\n",
    "np.fill_diagonal(winning_diffs, np.nan)\n",
    "\n",
    "\n",
    "\n",
    "#winning_percentage[\"mean\"] = winning_percentage.values.mean(axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "np.fill_diagonal(winning_percentage.values, np.nan)\n",
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "ax.pcolormesh(winning_diffs, cmap=custom_cmap, vmin=0, vmax=1, alpha=0.3, edgecolors=\"white\", linewidth=2, shading=\"flat\")\n",
    "ax.set_aspect(\"auto\")\n",
    "for i in range(winning_percentage.shape[0]):\n",
    "    for j in range(winning_percentage.shape[1]):\n",
    "        formatted_text = r'${:.2f}$'.format(winning_percentage.iloc[i, j]).replace(\"$nan$\", \"N/A\")\n",
    "        plt.text(j+0.5, i+0.55, formatted_text, ha=\"center\", va=\"center\", color=\"black\", fontsize=18)#, bbox=dict(facecolor='white', edgecolor='none', pad=0))\n",
    "ax.set_xticks(np.arange(len(winning_percentage))+0.5, winning_percentage.index.values, fontsize=15)\n",
    "ax.set_yticks(np.arange(len(winning_percentage))+0.5, winning_percentage.index.values, fontsize=15)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"approaches_winning_matrix_{LOSS_FUNC}.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f59f03-9ead-4cdd-8602-eef8d906bd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "import re\n",
    "from itertools import combinations\n",
    "from scipy.stats import wilcoxon\n",
    "from statsmodels.stats.multitest import multipletests \n",
    "\n",
    "# Specifiy test case, which is either \"all\" or one of the LFC approaches.\n",
    "test_approach = \"all\"\n",
    "alpha = 0.05\n",
    "ordered_criteria = list(subsets.keys())\n",
    "ordered_criteria.insert(1, \"DEF-DATA\")\n",
    "ordered_criteria.insert(2, \"DEF\")\n",
    "\n",
    "# Combine all datasets into one DataFrame.\n",
    "df_mean_dict = {k: v[\"mean\"][[\"DEF\", \"DEF-DATA\"] + list(subsets.keys())] for k, v in df_dict.items()}\n",
    "data_all = pd.concat(df_mean_dict, names=['dataset', 'variant'])\n",
    "if test_approach != \"all\":\n",
    "    is_approach = [test_approach in idx_tuple[1] for idx_tuple in data_all.index]\n",
    "    data_all = data_all[is_approach]\n",
    "if data_all.isnull().any().any():\n",
    "    data_all = data_all.dropna()\n",
    "data_all = data_all.reset_index()\n",
    "data_long = data_all.melt(id_vars=['dataset', 'variant'], var_name='approach', value_name='loss')\n",
    "data_long['block'] = data_long['dataset'].astype(str) + \"_\" + data_long['variant'].astype(str)\n",
    "data_long = data_long[[\"block\", \"approach\", \"loss\"]]\n",
    "data_long['block_id'] = data_long.groupby('block').ngroup()\n",
    "wide = data_long.pivot(index='block', columns='approach', values='loss')\n",
    "\n",
    "# Perform paired Wilcoxon signed rank test.\n",
    "approaches = wide.columns\n",
    "n_approaches = len(approaches)\n",
    "p_matrix = pd.DataFrame(np.ones((n_approaches, n_approaches)), index=approaches, columns=approaches)\n",
    "sign_matrix = pd.DataFrame(np.zeros((n_approaches, n_approaches)), index=approaches, columns=approaches)\n",
    "p_list, approach_pair_list = [], []\n",
    "for i, j in combinations(approaches, 2):\n",
    "    _, p = wilcoxon(wide[i].values, wide[j].values, alternative=\"two-sided\")\n",
    "    p_list.append(p)\n",
    "    approach_pair_list.append((i, j))\n",
    "    diffs = wide[i].values - wide[j].values\n",
    "    wins = (diffs < 0).sum()\n",
    "    losses = (diffs > 0).sum() \n",
    "    if wins > losses:\n",
    "        sign_matrix.loc[i, j] = 1\n",
    "        sign_matrix.loc[j, i] = -1\n",
    "    elif wins < losses:\n",
    "        sign_matrix.loc[i, j] = -1\n",
    "        sign_matrix.loc[j, i] = 1\n",
    "p_adj_list = multipletests(p_list, method=\"holm\")[1]\n",
    "for (i, j), p_adj in zip(approach_pair_list, p_adj_list):\n",
    "    p_matrix.loc[i, j] = p_matrix.loc[j, i] = p_adj\n",
    "sign_matrix.values[:] = np.where(p_matrix.values < alpha, -1*sign_matrix.values, 0)\n",
    "    \n",
    "    \n",
    "# Print critical difference diagram.\n",
    "avg_rank = data_long.groupby('block')[\"loss\"].rank(pct=False, method=\"average\").groupby(data_long[\"approach\"]).mean()[ordered_criteria]\n",
    "std_rank = data_long.groupby('block')[\"loss\"].rank(pct=False, method=\"average\").groupby(data_long[\"approach\"]).sem()[ordered_criteria]\n",
    "print(\"Average ranks:\")\n",
    "print(avg_rank.round(2))\n",
    "print(\"Standard deviation of ranks:\")\n",
    "print(std_rank.round(2))\n",
    "plt.figure(figsize=(10, 3), dpi=100)\n",
    "plt.title('Critical difference diagram of average score ranks')\n",
    "ax = sp.critical_difference_diagram(avg_rank, (p_matrix >= 0.05).astype(float), text_h_margin=0.3, label_fmt_left='{label} ({rank:.2f})', label_fmt_right='({rank:.2f}) {label}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"cd_plot_criteria_{test_approach}.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Plot matrix of p-values.\n",
    "plt.figure(figsize=(10, 5))\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"bone\", colors)\n",
    "plt.imshow(sign_matrix, cmap=custom_cmap, vmin=-1, vmax=1, alpha=0.5, aspect=\"auto\")\n",
    "n_rows, n_cols = p_matrix.shape\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        val = p_matrix.iloc[i, j]\n",
    "        if np.isnan(val):\n",
    "            continue\n",
    "        txt = f\"{val:.4f}\"\n",
    "        txt = re.sub(r\"^(-?)0\\.\", r\"\\1.\", txt)\n",
    "        plt.text(i, j, txt, ha=\"center\", va=\"center\", color='black', fontsize=16)\n",
    "\n",
    "plt.xticks(np.arange(n_rows), p_matrix.columns, rotation=0)\n",
    "plt.tick_params(top=False, labeltop=False, bottom=True, labelbottom=True)\n",
    "plt.yticks(np.arange(n_cols), p_matrix.columns)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"p_matrix_criteria.pdf\")\n",
    "plt.show()\n",
    "\n",
    "gains = wide.copy()\n",
    "gains.iloc[:] = ((wide[\"DEF\"].values[:, None] - wide.values))# / wide[\"DEF\"].values[:, None]) * 100\n",
    "print(gains[ordered_criteria].mean(axis=0).round(2))\n",
    "print(gains[ordered_criteria].sem(axis=0).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1f0b5-ddfd-4fa3-8e8d-2756b6191f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specifiy test case, which is either \"all\" or one of the model selection criteria.\n",
    "test_approach = \"ENS\"\n",
    "alpha = 0.05\n",
    "ordered_approaches = APPROACHES[1:]\n",
    "\n",
    "# Combine all datasets into one DataFrame.\n",
    "rows = []\n",
    "for ds_name, v in df_dict.items():\n",
    "    wide = v[\"mean\"][[\"DEF\", \"DEF-DATA\"] + list(subsets.keys())]\n",
    "    wide.loc[\"majority-vote\"].fillna(wide.loc[\"majority-vote\"][\"AEU\"], inplace=True)\n",
    "    wide = wide.assign(variant=wide.index)                     # <─ key change\n",
    "    wide.loc[\"majority-vote-def\"] = v[\"mean\"][\"DEF\"].loc[\"majority-vote\"]\n",
    "    wide.loc[\"majority-vote-def\", \"variant\"] = \"majority-vote-def\"\n",
    "    long = wide.melt(id_vars=\"variant\",\n",
    "                     var_name=\"approach\",\n",
    "                     value_name=\"loss\")\n",
    "\n",
    "    long[\"dataset\"] = ds_name\n",
    "    rows.append(long)\n",
    "data_long = pd.concat(rows, ignore_index=True)\n",
    "data_long = data_long.loc[\"ground-truth\" != data_long[\"variant\"]]\n",
    "if test_approach != \"all\":\n",
    "    is_approach = data_long[\"approach\"] == test_approach\n",
    "    data_long = data_long[is_approach]\n",
    "\n",
    "if test_approach != \"all\":\n",
    "    data_long[\"block\"] = data_long[\"dataset\"]\n",
    "else:\n",
    "    data_long[\"block\"] = data_long[\"dataset\"] + \"_\" + data_long[\"approach\"]\n",
    "data_long = data_long.dropna(subset=[\"loss\"])\n",
    "data_long[\"block_id\"] = data_long.groupby(\"block\").ngroup()\n",
    "wide = data_long.pivot(index='block', columns='variant', values='loss')\n",
    "\n",
    "# Perform paired Wilcoxon signed rank test.\n",
    "approaches = wide.columns\n",
    "n_approaches = len(approaches)\n",
    "p_matrix = pd.DataFrame(np.ones((n_approaches, n_approaches)), index=approaches, columns=approaches)\n",
    "sign_matrix = pd.DataFrame(np.zeros((n_approaches, n_approaches)), index=approaches, columns=approaches)\n",
    "p_list, approach_pair_list = [], []\n",
    "for i, j in combinations(approaches, 2):\n",
    "    _, p = wilcoxon(wide[i].values, wide[j].values, alternative=\"two-sided\")\n",
    "    p_list.append(p)\n",
    "    approach_pair_list.append((i, j))\n",
    "    diffs = wide[i].values - wide[j].values\n",
    "    wins = (diffs < 0).sum()\n",
    "    losses = (diffs > 0).sum() \n",
    "    if wins > losses:\n",
    "        sign_matrix.loc[i, j] = 1\n",
    "        sign_matrix.loc[j, i] = -1\n",
    "    elif wins < losses:\n",
    "        sign_matrix.loc[i, j] = -1\n",
    "        sign_matrix.loc[j, i] = 1\n",
    "p_adj_list = multipletests(p_list, method=\"holm\")[1]\n",
    "for (i, j), p_adj in zip(approach_pair_list, p_adj_list):\n",
    "    p_matrix.loc[i, j] = p_matrix.loc[j, i] = p_adj\n",
    "    \n",
    "sign_matrix.values[:] = np.where(p_matrix.values < alpha, -1*sign_matrix.values, 0)\n",
    "\n",
    "# Print critical difference diagram.\n",
    "avg_rank = data_long.groupby('block')[\"loss\"].rank(pct=False, method=\"average\").groupby(data_long[\"variant\"]).mean()\n",
    "std_rank = data_long.groupby('block')[\"loss\"].rank(pct=False, method=\"average\").groupby(data_long[\"variant\"]).sem()\n",
    "print(\"Average ranks:\")\n",
    "print(avg_rank[ordered_approaches].round(2))\n",
    "print(\"Standard deviation of ranks:\")\n",
    "print(std_rank[ordered_approaches].round(2))\n",
    "plt.figure(figsize=(10, 3), dpi=100)\n",
    "plt.title('Critical difference diagram of average score ranks')\n",
    "ax = sp.critical_difference_diagram(avg_rank, (p_matrix >= 0.05).astype(float), text_h_margin=0.3, label_fmt_left='{label} ({rank:.2f})', label_fmt_right='({rank:.2f}) {label}')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"cd_plot_approaches_{test_approach}.pdf\")\n",
    "plt.show()\n",
    "\n",
    "# Plot matrix of p-values.\n",
    "plt.figure(figsize=(12, 5))\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"bone\", colors)\n",
    "plt.imshow(sign_matrix, cmap=custom_cmap, vmin=-1, vmax=1, alpha=0.5, aspect=\"auto\")\n",
    "n_rows, n_cols = p_matrix.shape\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        val = p_matrix.iloc[i, j]\n",
    "        if np.isnan(val):\n",
    "            continue\n",
    "        txt = f\"{val:.4f}\"\n",
    "        txt = re.sub(r\"^(-?)0\\.\", r\"\\1.\", txt)\n",
    "        plt.text(i, j, txt, ha=\"center\", va=\"center\", color='black', fontsize=16)\n",
    "\n",
    "plt.xticks(np.arange(n_rows), p_matrix.columns, rotation=0)\n",
    "plt.tick_params(top=False, labeltop=False, bottom=True, labelbottom=True)\n",
    "plt.yticks(np.arange(n_cols), p_matrix.columns)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"p_matrix_approaches.pdf\")\n",
    "plt.show()\n",
    "\n",
    "gains = wide.copy()\n",
    "gains.iloc[:] = ((wide[\"majority-vote-def\"].values[:, None] - wide.values) / wide[\"majority-vote-def\"].values[:, None]) * 100\n",
    "print(gains[ordered_approaches].mean(axis=0).round(2))\n",
    "print(gains[ordered_approaches].sem(axis=0).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cccc8-884e-4a5c-aeba-134c96f986d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "\n",
    "rank_matrices = []\n",
    "criteria = [\"TRUE\", \"DEF\", \"DEF-DATA\", \"ENS\"]\n",
    "for test_approach in criteria:\n",
    "    alpha = 0.05\n",
    "    ordered_approaches = APPROACHES[1:]\n",
    "\n",
    "    # Combine all datasets into one DataFrame.\n",
    "    rows = []\n",
    "    for ds_name, v in df_dict.items():\n",
    "        wide = v[\"mean\"][[\"DEF\", \"DEF-DATA\"] + list(subsets.keys())]\n",
    "        wide.loc[\"majority-vote\"].fillna(wide.loc[\"majority-vote\"][\"AEU\"], inplace=True)\n",
    "        wide = wide.assign(variant=wide.index)                     # <─ key change\n",
    "        #wide.loc[\"majority-vote-def\"] = v[\"mean\"][\"DEF\"].loc[\"majority-vote\"]\n",
    "        #wide.loc[\"majority-vote-def\", \"variant\"] = \"majority-vote-def\"\n",
    "        long = wide.melt(id_vars=\"variant\",\n",
    "                         var_name=\"approach\",\n",
    "                         value_name=\"loss\")\n",
    "\n",
    "        long[\"dataset\"] = ds_name\n",
    "        rows.append(long)\n",
    "    data_long = pd.concat(rows, ignore_index=True)\n",
    "    data_long = data_long.loc[\"ground-truth\" != data_long[\"variant\"]]\n",
    "    if test_approach != \"all\":\n",
    "        is_approach = data_long[\"approach\"] == test_approach\n",
    "        data_long = data_long[is_approach]\n",
    "\n",
    "    if test_approach != \"all\":\n",
    "        data_long[\"block\"] = data_long[\"dataset\"]\n",
    "    else:\n",
    "        data_long[\"block\"] = data_long[\"dataset\"] + \"_\" + data_long[\"approach\"]\n",
    "    data_long = data_long.dropna(subset=[\"loss\"])\n",
    "    data_long[\"block_id\"] = data_long.groupby(\"block\").ngroup()\n",
    "    wide = data_long.pivot(index='block', columns='variant', values='loss')\n",
    "    # Plot ranking values.\n",
    "    avg_rank = data_long.groupby('block')[\"loss\"].rank(pct=False, method=\"average\")\n",
    "    rank_matrices.append(wide.rank(pct=False, method=\"average\", axis=\"columns\"))\n",
    "R = np.stack(rank_matrices, axis=1)\n",
    "\n",
    "pairs  = list(itertools.combinations(range(len(criteria)), 2))\n",
    "tau    = np.zeros((35, len(pairs)))      # 35 datasets × 6 pairs\n",
    "for d in range(35):\n",
    "    for k, (c1, c2) in enumerate(pairs):\n",
    "        tau[d, k], _ = kendalltau(R[d, c1], R[d, c2])\n",
    "        \n",
    "# Heat-map of mean τ\n",
    "mean_tau = tau.mean(axis=0)\n",
    "mean_sem = tau.std(axis=0) / (tau.shape[0]-1)\n",
    "M = np.full((len(criteria), len(criteria)), 1.0)\n",
    "for (k,(i,j)) in enumerate(pairs):\n",
    "    M[i,j] = M[j,i] = mean_tau[k]\n",
    "sns.heatmap(M, annot=True, vmin=-1, vmax=1, cmap=custom_cmap,\n",
    "            xticklabels=criteria,\n",
    "            yticklabels=criteria)\n",
    "plt.title(\"Mean Kendall τ between criteria\");\n",
    "plt.show()\n",
    "\n",
    "# Boxplot of τ distributions\n",
    "plt.figure(figsize=(5, 6))\n",
    "plt.axvline(0, color='grey', linestyle=':')\n",
    "for (k,(i,j)) in enumerate(pairs):\n",
    "    plt.scatter(tau[:, k], np.full_like(tau[:, k], fill_value=k+1), c=\"blue\", s=8)\n",
    "plt.violinplot(tau, showmeans=True, bw_method=0.3, vert=False)\n",
    "plt.yticks(range(1, len(pairs)+1), [f\"{i+1}–{j+1}\" for i,j in pairs])\n",
    "plt.xticks(np.arange(-1, 1.25, 0.25))\n",
    "plt.xlabel(\"Kendall τ\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"kendall_violinplot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00f0f3-85b8-4382-815f-749661e2449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SETTINGS\n",
    "# ------------------------------------------------------------------\n",
    "scatter_type = \"relative\"\n",
    "approaches      = list(subsets.keys())\n",
    "approaches.insert(1, \"DEF-DATA\")\n",
    "approaches.insert(2, \"DEF\")\n",
    "baseline        = \"DEF\"\n",
    "comp_approaches = [a for a in approaches if a != baseline]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 · COLLECT GAPS PER GROUP\n",
    "# ------------------------------------------------------------------\n",
    "abs_by_group = {}   # {group: {approach: [gaps]}}\n",
    "rel_by_group = {}   # same for % gaps\n",
    "\n",
    "for ds_name, df in df_dict.items():\n",
    "    # sub: DataFrame whose *rows are groups* and columns are the approaches\n",
    "    sub = df[\"mean\"][approaches]\n",
    "\n",
    "    for group_name, row in sub.iterrows():\n",
    "        base_val = row[baseline]\n",
    "        if pd.isnull(base_val):\n",
    "            continue\n",
    "\n",
    "        # make sure the inner dicts exist\n",
    "        abs_by_group.setdefault(group_name, {a: [] for a in comp_approaches})\n",
    "        rel_by_group.setdefault(group_name, {a: [] for a in comp_approaches})\n",
    "\n",
    "        for a in comp_approaches:\n",
    "            val = row[a]\n",
    "            if pd.notnull(val):\n",
    "                gap     = base_val - val                      # +ve ⇒ better than DEF\n",
    "                gap_pct = (gap / base_val) * 100\n",
    "                abs_by_group[group_name][a].append(gap)\n",
    "                rel_by_group[group_name][a].append(gap_pct)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2 · BUILD SUMMARY TABLE  (mean ± SD)\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "for group in sorted(abs_by_group):                      # one row per group\n",
    "    row_dict = {\"group\": group}\n",
    "    for a in comp_approaches:\n",
    "        abs_vals = abs_by_group[group][a]\n",
    "        rel_vals = rel_by_group[group][a]\n",
    "\n",
    "        # If the group lacks observations for an approach, fill with NaN\n",
    "        if abs_vals:\n",
    "            abs_mean = np.mean(abs_vals)\n",
    "            abs_sd   = np.std(abs_vals, ddof=1) / len(abs_vals)\n",
    "            rel_mean = np.mean(rel_vals)\n",
    "            rel_sd   = np.std(rel_vals, ddof=1) / len(abs_vals)\n",
    "        else:\n",
    "            abs_mean = abs_sd = rel_mean = rel_sd = np.nan\n",
    "\n",
    "        # column names like \"AEU_abs_mean\", \"AEU_abs_sd\", ...\n",
    "        row_dict[f\"{a}_abs_m\"] = abs_mean\n",
    "        row_dict[f\"{a}_abs_s\"]   = abs_sd\n",
    "        row_dict[f\"{a}_rel_m\"] = rel_mean\n",
    "        row_dict[f\"{a}_rel_s\"]   = rel_sd\n",
    "\n",
    "    rows.append(row_dict)\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# nicer column order: group first, then each approach’s stats\n",
    "ordered_cols = (\n",
    "    [\"group\"] +\n",
    "    [f\"{a}_{suffix}\"\n",
    "     for a in comp_approaches\n",
    "     for suffix in (\"abs_m\", \"abs_s\", \"rel_m\", \"rel_s\")]\n",
    ")\n",
    "summary_df = summary_df[ordered_cols]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3 · DISPLAY OR SAVE\n",
    "# ------------------------------------------------------------------\n",
    "pd.set_option(\"display.float_format\", \"{:,.4f}\".format)\n",
    "#print(summary_df.to_markdown(tablefmt=\"github\", floatfmt=\".2f\"))            # to screen\n",
    "\n",
    "\n",
    "categories      = list(abs_by_group.keys())[1:]       # 12 categorical groups\n",
    "n_groups        = len(categories)\n",
    "n_approaches    = len(comp_approaches)\n",
    "\n",
    "x_base          = np.arange(n_groups)                # positions on x‑axis\n",
    "offset          = 0.8 / n_approaches                  # spread approaches across the group slot\n",
    "markers         = [\"o\", \"v\", \"s\"]#, \"^\", \"v\", \"P\", \"X\"] # enough unique markers\n",
    "legend = {\n",
    "    \"TRUE\": [\"o\", \"#949494ff\"],\n",
    "    \"DEF\": [\"v\", \"#949494ff\"],\n",
    "    \"DEF-DATA\": [\"s\", \"#949494ff\"],\n",
    "    \n",
    "    \"AEU\": [\"o\", \"#4d4da6ff\"],\n",
    "    \"AEC\": [\"v\", \"#4d4da6ff\"],\n",
    "    \"ALU\": [\"s\", \"#4d4da6ff\"],\n",
    "    \"ALC\": [\"D\", \"#4d4da6ff\"],\n",
    "    \n",
    "    \"CXU\": [\"o\", \"#a64da6ff\"],\n",
    "    \"CEC\": [\"v\", \"#a64da6ff\"],\n",
    "    \"CLC\": [\"s\", \"#a64da6ff\"],\n",
    "    \n",
    "    \"ENS\": [\"s\", \"#4da6a6ff\"],\n",
    "}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(max(6, n_groups * 1.5), 5))\n",
    "\n",
    "for idx, approach in enumerate(comp_approaches):\n",
    "    means, ses = [], []\n",
    "    for g in categories:\n",
    "        if scatter_type == \"relative\":\n",
    "            vals = rel_by_group[g][approach]\n",
    "            x_ticks = np.arange(-10, 30, 5)\n",
    "        else:\n",
    "            vals = abs_by_group[g][approach]\n",
    "            x_ticks = np.arange(-4, 10, 2)\n",
    "        if vals:\n",
    "            means.append(np.mean(vals))\n",
    "            ses.append(np.std(vals, ddof=1) / np.sqrt(len(vals)))\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            ses.append(np.nan)\n",
    "\n",
    "    # x positions shifted per approach\n",
    "    x_pos = x_base - 0.4 + offset/2 + idx * offset\n",
    "    plt.errorbar(x_pos, means, yerr=ses, fmt=legend[approach][0], color=legend[approach][1],\n",
    "                 capsize=3, label=approach, linestyle='None')\n",
    "\n",
    "# Aesthetics\n",
    "plt.axhline(0, color='grey', linestyle=':')\n",
    "plt.xlim(-0.5, n_groups-0.5)\n",
    "plt.xticks(x_base, categories, ha='center', fontsize=15)\n",
    "plt.yticks(x_ticks, fontsize=15)\n",
    "plt.legend(ncol=min(n_approaches, 11), fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"criteria_{scatter_type}_diff_per_approach.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdfeec-312e-4a6b-b3c5-98f253c5f532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_set_meta_features = pd.read_csv(\"../python_scripts/metafeatures.csv\", index_col=\"dataset\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SETTINGS\n",
    "# ------------------------------------------------------------------\n",
    "scatter_type = \"absolute\"\n",
    "criteria      = list(subsets.keys())\n",
    "criteria.insert(1, \"DEF-DATA\")\n",
    "criteria.insert(2, \"DEF\")\n",
    "baseline        = \"DEF\"\n",
    "comp_criteria = [c for c in criteria if c != baseline]\n",
    "x_label = \"aggregation_noise\"\n",
    "keep_indices = []\n",
    "abs_by_group = {c: [] for c in comp_criteria}\n",
    "rel_by_group = {c: [] for c in comp_criteria}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 · COLLECT GAPS PER GROUP\n",
    "# ------------------------------------------------------------------\n",
    "for ds_idx, (ds_name, ds_features) in enumerate(data_set_meta_features.iterrows()):\n",
    "    if ds_name not in df_dict:\n",
    "        continue\n",
    "    keep_indices.append(ds_idx)\n",
    "    sub = df_dict[ds_name][\"mean\"][criteria]\n",
    "    sub = sub.dropna(how='any')\n",
    "    x_val = ds_features[x_label]\n",
    "\n",
    "    abs_diffs = sub[baseline].values[:, None] - sub[comp_criteria].values\n",
    "    rel_diffs = (abs_diffs / sub[baseline].values[:, None]) * 100\n",
    "    for criterion_idx, criterion in enumerate(comp_criteria):\n",
    "        abs_by_group[criterion].append(abs_diffs[:, criterion_idx].ravel().tolist())\n",
    "        rel_by_group[criterion].append(rel_diffs[:, criterion_idx].ravel().tolist())\n",
    "\n",
    "# Scatter plot\n",
    "legend = {\n",
    "    \"TRUE\": [\"o\", \"#949494ff\"],\n",
    "    \"DEF\": [\"v\", \"#949494ff\"],\n",
    "    \"DEF-DATA\": [\"s\", \"#949494ff\"],\n",
    "    \n",
    "    \"AEU\": [\"o\", \"#4d4da6ff\"],\n",
    "    \"AEA\": [\"v\", \"#4d4da6ff\"],\n",
    "    \"ALU\": [\"s\", \"#4d4da6ff\"],\n",
    "    \"ALA\": [\"D\", \"#4d4da6ff\"],\n",
    "    \n",
    "    \"CXU\": [\"o\", \"#a64da6ff\"],\n",
    "    \"CEA\": [\"v\", \"#a64da6ff\"],\n",
    "    \"CLA\": [\"s\", \"#a64da6ff\"],\n",
    "    \n",
    "    \"ENS\": [\"s\", \"#4da6a6ff\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Compute bin edges and indices\n",
    "bins = np.array([0, 10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "num_bins = len(bins)-1\n",
    "x_values = data_set_meta_features[x_label].values[keep_indices]\n",
    "bin_indices = np.digitize(x_values, bins) - 1  # subtract 1 to get 0-based index\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "print(f\"Bins: {np.unique(bin_indices, return_counts=True)}\")\n",
    "\n",
    "\n",
    "x_base          = np.arange(num_bins)                \n",
    "offset          = 0.8 / len(comp_criteria)\n",
    "plt.figure(figsize=(max(6, len(comp_criteria) * 1.5), 5))\n",
    "for criterion_idx, criterion in enumerate(comp_criteria):\n",
    "    means, ses = [], []\n",
    "    y = np.array(abs_by_group[criterion]) if scatter_type == \"absolute\" else np.array(rel_by_group[criterion])\n",
    "    for bin_idx, bin_center in enumerate(bin_centers):\n",
    "        y_ravel = y[bin_indices == bin_idx].ravel()\n",
    "        means.append(y_ravel.mean())\n",
    "        ses.append(y_ravel.std() / np.sqrt(len(y_ravel)))\n",
    "        y_ticks = np.arange(-4, 10, 2)\n",
    "\n",
    "    # x positions shifted per approach\n",
    "    x_pos = x_base - 0.4 + offset/2 + criterion_idx * offset\n",
    "    plt.errorbar(x_pos, means, yerr=ses, fmt=legend[criterion][0], color=legend[criterion][1],\n",
    "                 capsize=3, label=criterion, linestyle='None')\n",
    "\n",
    "# Aesthetics\n",
    "plt.axhline(0, color='grey', linestyle=':')\n",
    "plt.xlim(-0.5, num_bins-0.5)\n",
    "plt.xticks(x_base, bin_centers, ha='center', fontsize=15)\n",
    "plt.yticks(y_ticks, fontsize=15)\n",
    "plt.legend(ncol=min(2, 11), fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"criteria_{scatter_type}_diff_per_noise_level.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9162d3-d6ba-48b6-a80f-2a8a32b3e0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# SETTINGS\n",
    "# ------------------------------------------------------------------\n",
    "scatter_type = \"absolute\"\n",
    "approaches      = APPROACHES[1:]\n",
    "baseline        = \"majority-vote\"\n",
    "comp_approaches = approaches\n",
    "criteria        = list(subsets.keys())\n",
    "criteria.insert(1, \"DEF-DATA\")\n",
    "criteria.insert(2, \"DEF\")\n",
    "print(criteria)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 · COLLECT GAPS PER GROUP\n",
    "# ------------------------------------------------------------------\n",
    "abs_by_group = {}   # {group: {approach: [gaps]}}\n",
    "rel_by_group = {}   # same for % gaps\n",
    "\n",
    "for ds_name, df in df_dict.items():\n",
    "    # sub: DataFrame whose *rows are groups* and columns are the approaches\n",
    "    sub = df[\"mean\"][criteria].T[approaches]\n",
    "    base_val = sub[\"majority-vote\"][\"DEF\"]\n",
    "\n",
    "    for group_name, row in sub.iterrows():\n",
    "        #base_val = row[baseline]\n",
    "        if pd.isnull(base_val):\n",
    "            continue\n",
    "\n",
    "        # make sure the inner dicts exist\n",
    "        abs_by_group.setdefault(group_name, {a: [] for a in comp_approaches})\n",
    "        rel_by_group.setdefault(group_name, {a: [] for a in comp_approaches})\n",
    "\n",
    "        for a in comp_approaches:\n",
    "            val = row[a]\n",
    "            if pd.notnull(val):\n",
    "                gap     = base_val - val                      # +ve ⇒ better than DEF\n",
    "                gap_pct = (gap / base_val) * 100\n",
    "                abs_by_group[group_name][a].append(gap)\n",
    "                rel_by_group[group_name][a].append(gap_pct)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2 · BUILD SUMMARY TABLE  (mean ± SD)\n",
    "# ------------------------------------------------------------------\n",
    "rows = []\n",
    "for group in sorted(abs_by_group):                      # one row per group\n",
    "    row_dict = {\"group\": group}\n",
    "    for a in comp_approaches:\n",
    "        abs_vals = abs_by_group[group][a]\n",
    "        rel_vals = rel_by_group[group][a]\n",
    "\n",
    "        # If the group lacks observations for an approach, fill with NaN\n",
    "        if abs_vals:\n",
    "            abs_mean = np.mean(abs_vals)\n",
    "            abs_sd   = np.std(abs_vals, ddof=1) / len(abs_vals)\n",
    "            rel_mean = np.mean(rel_vals)\n",
    "            rel_sd   = np.std(rel_vals, ddof=1) / len(abs_vals)\n",
    "        else:\n",
    "            abs_mean = abs_sd = rel_mean = rel_sd = np.nan\n",
    "\n",
    "        # column names like \"AEU_abs_mean\", \"AEU_abs_sd\", ...\n",
    "        row_dict[f\"{a}_abs_m\"] = abs_mean\n",
    "        row_dict[f\"{a}_abs_s\"]   = abs_sd\n",
    "        row_dict[f\"{a}_rel_m\"] = rel_mean\n",
    "        row_dict[f\"{a}_rel_s\"]   = rel_sd\n",
    "\n",
    "    rows.append(row_dict)\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "# nicer column order: group first, then each approach’s stats\n",
    "ordered_cols = (\n",
    "    [\"group\"] +\n",
    "    [f\"{a}_{suffix}\"\n",
    "     for a in comp_approaches\n",
    "     for suffix in (\"abs_m\", \"abs_s\", \"rel_m\", \"rel_s\")]\n",
    ")\n",
    "summary_df = summary_df[ordered_cols]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3 · DISPLAY OR SAVE\n",
    "# ------------------------------------------------------------------\n",
    "pd.set_option(\"display.float_format\", \"{:,.4f}\".format)\n",
    "#print(summary_df.to_markdown(tablefmt=\"github\", floatfmt=\".2f\"))            # to screen\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4 · COLLAPSE mean & sd  →  \"$μ_{\\pm σ}$\"\n",
    "# ------------------------------------------------------------------\n",
    "def make_cell(mu, sd):\n",
    "    if np.isnan(mu) or np.isnan(sd):\n",
    "        return \"--\"\n",
    "    if mu > 0:\n",
    "        return fr\"$+{mu:.4f}_{{\\pm {sd:.4f}}}$\"\n",
    "    else:\n",
    "        return fr\"${mu:.4f}_{{\\pm {sd:.4f}}}$\"\n",
    "\n",
    "# If they don’t, raise a gentle error.\n",
    "if 'abs_by_group' not in globals():\n",
    "    raise NameError(\"`abs_by_group` not found – run the gap‑collection cell first.\")\n",
    "\n",
    "categories      = abs_by_group.keys()        # 12 categorical groups\n",
    "n_groups        = len(categories)\n",
    "print(n_groups)\n",
    "n_approaches    = len(comp_approaches)\n",
    "\n",
    "x_base          = np.arange(n_groups)                # positions on x‑axis\n",
    "offset          = 0.8 / n_approaches                  # spread approaches across the group slot\n",
    "markers         = [\"o\", \"v\", \"s\"]#, \"^\", \"v\", \"P\", \"X\"] # enough unique markers\n",
    "legend = {\n",
    "    \"majority-vote\": [\"o\", \"#949494ff\"],\n",
    "    \"dawid-skene\": [\"v\", \"#949494ff\"],\n",
    "    \n",
    "    \"crowd-layer\": [\"o\", \"#4d4da6ff\"],\n",
    "    \"trace-reg\": [\"v\", \"#4d4da6ff\"],\n",
    "    \"conal\": [\"s\", \"#4d4da6ff\"],\n",
    "    \"union-net-a\": [\"D\", \"#4d4da6ff\"],\n",
    "    \"union-net-b\": [\"X\", \"#4d4da6ff\"],\n",
    "    \"geo-reg-w\": [\"P\", \"#4d4da6ff\"],\n",
    "    \"geo-reg-f\": [\"^\", \"#4d4da6ff\"],\n",
    "    \n",
    "    \"crowd-ar\": [\"o\", \"#a64da6ff\"],\n",
    "    \"madl\": [\"v\", \"#a64da6ff\"],\n",
    "    \"annot-mix\": [\"s\", \"#a64da6ff\"],\n",
    "    \"coin-net\": [\"D\", \"#a64da6ff\"],\n",
    "}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(max(6, n_groups * 1.5), 5))\n",
    "\n",
    "for idx, approach in enumerate(comp_approaches):\n",
    "    means, ses = [], []\n",
    "    for g in ordered_criteria:\n",
    "        if scatter_type == \"relative\":\n",
    "            vals = rel_by_group[g][approach]\n",
    "            yticks = np.arange(-5, 40, 5)\n",
    "        else:\n",
    "            vals = abs_by_group[g][approach]\n",
    "            yticks = np.arange(-2, 14, 2)\n",
    "        if vals:\n",
    "            means.append(np.mean(vals))\n",
    "            ses.append(np.std(vals, ddof=1) / np.sqrt(len(vals)))\n",
    "        else:\n",
    "            means.append(np.nan)\n",
    "            ses.append(np.nan)\n",
    "\n",
    "    # x positions shifted per approach\n",
    "    x_pos = x_base - 0.4 + offset/2 + idx * offset\n",
    "    plt.errorbar(x_pos, means, yerr=ses, fmt=legend[approach][0], color=legend[approach][1],\n",
    "                 capsize=3, label=approach, linestyle='None')\n",
    "\n",
    "# Aesthetics\n",
    "plt.axhline(0, color='grey', linestyle=':')\n",
    "plt.xlim(-0.5, n_groups-0.5)\n",
    "plt.xticks(x_base, categories, ha='center', fontsize=15)\n",
    "plt.yticks(yticks, fontsize=15)\n",
    "plt.legend(ncol=min(n_approaches, 6), fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"approaches_{scatter_type}_diff_per_criterion.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9172f-3d31-4cef-b77b-3c0518a9cc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_times = np.array([154.3700, 153.3405, 154.2420, 154.5105, 154.6445])\n",
    "risk_measurement_times = np.array([1.7215, 1.6655, 1.6910, 1.6225, 1.6180])\n",
    "\n",
    "# Calculate means\n",
    "means = [training_times.mean(), risk_measurement_times.mean()]\n",
    "x_labels = ['Training Times', 'Risk Measurement Times']\n",
    "\n",
    "# Create vertical bar plot\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "x_pos = range(len(labels))\n",
    "ax.bar(x_pos, means)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(x_labels)\n",
    "ax.set_yticks(np.arange(0, 200, 30))\n",
    "ax.set_ylabel('Mean Time')\n",
    "\n",
    "# Annotate bars with mean values\n",
    "for i, v in enumerate(means):\n",
    "    ax.text(i, v, f'${v:.2f}$s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"computation_time_comparison.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668daea1-8e9e-4993-b651-6c894034b6f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from typing import List, Sequence, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from typing import List, Tuple\n",
    "\n",
    "df_dict = {}\n",
    "approach_rankings = None\n",
    "metric_rankings = None\n",
    "approach_list = [\n",
    "    #\"ground_truth\",\n",
    "    #\"majority_vote\",\n",
    "    \"dawid-skene\",\n",
    "    \"crowd_layer\",\n",
    "    \"trace_reg\",\n",
    "    \"conal\",\n",
    "    \"union_net_a\",\n",
    "    \"union_net_b\",\n",
    "    \"madl\",\n",
    "    \"geo_reg_w\",\n",
    "    \"geo_reg_f\",\n",
    "    \"crowd_ar\",\n",
    "    \"annot_mix\",\n",
    "    \"coin_net\",\n",
    "]\n",
    "\n",
    "dataset_list = []\n",
    "for v in [\"worst-1\", \"worst-2\", \"worst-var\", \"rand-1\", \"rand-2\", \"rand-var\", \"full\"]:\n",
    "    dataset_list.extend(\n",
    "        [\n",
    "        f\"spc_{v}\",\n",
    "        f\"reuters_{v}\",\n",
    "        f\"music_genres_{v}\",\n",
    "        f\"label_me_{v}\",\n",
    "        f\"dopanim_{v}\",\n",
    "        ]\n",
    "    )\n",
    "for approach in approach_list:\n",
    "    df_dict[approach] = {}\n",
    "    for ds in dataset_list:\n",
    "        results_df_dict = {}\n",
    "        # Load results per dataset.\n",
    "        exp_name = f\"hyperparameter_search_{ds}\"\n",
    "        runs_df = evaluate(\n",
    "            mlruns_path=os.path.join(MLRUNS_PATH, exp_name),\n",
    "            experiment_name=exp_name,\n",
    "            update_columns=UPDATE_COLUMNS,\n",
    "            perf_type=\"class\",\n",
    "            version=\"valid\",\n",
    "            epoch=\"full\",\n",
    "            loss_func=LOSS_FUNC,\n",
    "            cache_path=CACHE_PATH,\n",
    "        )\n",
    "        if runs_df is None:\n",
    "            continue\n",
    "\n",
    "        # Preprocess columns.\n",
    "        df = runs_df.drop(columns=[\"data\"])\n",
    "        df['clf'] = df['clf'].astype(str) + '-' + df['agg'].astype(str)\n",
    "        df.drop(columns=['agg'], inplace=True)\n",
    "\n",
    "        # Identify '_valid' columns & rank them within each group\n",
    "        app = f\"aggregate-{approach}\" if approach == \"dawid-skene\" else f\"{approach}-None\"\n",
    "        df_subset = df[df[\"clf\"] == app]\n",
    "        df_dict[approach][ds] = df_subset\n",
    "        continue\n",
    "        \n",
    "print(df_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374c87d-73df-433f-9433-8da502e18973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil, floor\n",
    "# -------------------------------------------------------------------\n",
    "# 1.  utilities\n",
    "# -------------------------------------------------------------------\n",
    "def simplex_grid(\n",
    "        K: int,\n",
    "        step: float      = 0.05,\n",
    "        w_min: float     = 0.1,\n",
    "        w_max: float     = 0.9,\n",
    ") -> Iterable[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evenly–spaced points on the K-simplex  Σ w_k = 1  with\n",
    "\n",
    "        w_min ≤ w_k ≤ w_max   for every k.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    K      : int      number of dimensions\n",
    "    step   : float    grid resolution (weights are multiples of `step`)\n",
    "    w_min  : float    lower bound  (0 ≤ w_min ≤ 1/K)\n",
    "    w_max  : float    upper bound  (1/K ≤ w_max ≤ 1)\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    ndarray shape (K,) – one feasible weight vector per iteration\n",
    "    \"\"\"\n",
    "    if not (0.0 <= w_min <= w_max <= 1.0):\n",
    "        raise ValueError(\"Require 0 ≤ w_min ≤ w_max ≤ 1.\")\n",
    "    if K * w_min - 1 > 1e-12 or K * w_max + 1e-12 < 1:\n",
    "        raise ValueError(\n",
    "            \"Bounds incompatible with Σw_k=1: need  K·w_min ≤ 1 ≤ K·w_max.\"\n",
    "        )\n",
    "\n",
    "    m        = int(round(1 / step))              # denominator of the grid\n",
    "    lo       = ceil(w_min * m)                   # min count per dim\n",
    "    hi       = floor(w_max * m)                  # max count per dim\n",
    "    for counts in product(range(lo, hi + 1), repeat=K):\n",
    "        if sum(counts) == m:                     # simplex condition\n",
    "            yield np.array(counts, dtype=float) / m\n",
    "\n",
    "\n",
    "def mean_test_loss(\n",
    "    w: np.ndarray,\n",
    "    R: np.ndarray,      # (n_datasets, B, K)  - ranks (1 = best)\n",
    "    L: np.ndarray,      # (n_datasets, B)     - 0-1 loss (lower = better)\n",
    ") -> float:\n",
    "    \"\"\"Average loss obtained when the weighted-rank rule w selects 1 config.\"\"\"\n",
    "    score   = np.tensordot(R, w, axes=(2, 0))       # (n_datasets, B)\n",
    "    chosen  = score.argmin(1)                       # lowest rank wins\n",
    "    return L[np.arange(L.shape[0]), chosen].mean()  # minimiser\n",
    "\n",
    "def mean_rank_error(\n",
    "    w: np.ndarray,\n",
    "    R: np.ndarray,     # shape = (n_datasets, B, K)  – predicted ranks (1 = best)\n",
    "    L: np.ndarray,     # shape = (n_datasets, B)     – ground-truth loss (lower = better)\n",
    "    metric: str = \"kendall\"      # \"kendall\" | \"spearman\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Average *ranking* error of the weighted rule w over all datasets.\n",
    "    Smaller is better (negative correlation == large positive agreement).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. score each config with the weighted-Borda rule\n",
    "    score = np.tensordot(R, w, axes=(2, 0))          # (n_datasets, B)\n",
    "\n",
    "    # 2. convert scores & true losses into full orderings (0 = best)\n",
    "    pred_ord = score.argsort(axis=1)\n",
    "    true_ord = L.argsort(axis=1)                     # lower loss → better\n",
    "\n",
    "    # 3. compute per-dataset rank correlation / distance\n",
    "    errs = []\n",
    "    for p, t in zip(pred_ord, true_ord):\n",
    "        if metric == \"kendall\":\n",
    "            # τb ∈ [-1,1]; negate so optimiser can *minimise*\n",
    "            errs.append(-stats.kendalltau(p, t, variant=\"b\").correlation)\n",
    "        elif metric == \"spearman\":\n",
    "            errs.append(-stats.spearmanr(p, t).correlation)\n",
    "        else:\n",
    "            raise ValueError(\"metric must be 'kendall' or 'spearman'\")\n",
    "    return float(np.nanmean(errs))     # mean across datasets\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2.  main optimiser\n",
    "# -------------------------------------------------------------------\n",
    "def optimise_weights_cv(\n",
    "    dfs: List[pd.DataFrame],\n",
    "    valid_cols: Sequence[str],\n",
    "    cv_splits: List[Tuple[Sequence[int], Sequence[int]]],\n",
    "    test_col: str                       = \"test_loss\",\n",
    "    grid_step: float                    = 0.05,\n",
    "    ascending_rank: bool                = True,\n",
    ") -> Tuple[np.ndarray, float, List[float]]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs         : list of length N, each DataFrame shape (B, K+1)\n",
    "    valid_cols  : names of the K validation-metric columns\n",
    "    test_col    : column containing **loss** (lower = better)\n",
    "    cv_splits   : list of (train_idx, valid_idx) tuples\n",
    "                  – exactly what sklearn splitters yield\n",
    "    grid_step   : mesh size for simplex grid (0.05 ⇒ 10 626 pts for K=5)\n",
    "    ascending_rank : True  → rank 1 = *smallest* validation score (loss)\n",
    "                     False → rank 1 = *largest*  validation score (accuracy)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w_bar       : ndarray (K,)   – averaged weight vector over folds\n",
    "    cv_mean     : float          – mean held-out loss across folds\n",
    "    fold_scores : list[float]    – held-out loss per fold\n",
    "    \"\"\"\n",
    "    N, K = len(dfs), len(valid_cols)\n",
    "\n",
    "    # 1.  convert validation scores → ranks (1 = best)\n",
    "    ranks, losses = [], []\n",
    "    for df in dfs:\n",
    "        rnk = df[valid_cols].rank(ascending=ascending_rank, method=\"min\").to_numpy(dtype=int)\n",
    "        ranks.append(rnk)                       # (B, K)\n",
    "        losses.append(df[test_col].to_numpy())  # (B,)\n",
    "\n",
    "    ranks  = np.stack(ranks)   # (N, B, K)\n",
    "    losses = np.stack(losses)  # (N, B)\n",
    "\n",
    "    # 2. prepare weight grid once\n",
    "    grid = list(simplex_grid(K, step=grid_step))\n",
    "\n",
    "    # 3. cross-validation loop\n",
    "    w_sum, fold_scores = np.zeros(K), []\n",
    "    tested_losses = []\n",
    "    for train_idx, valid_idx in cv_splits:\n",
    "        R_tr, L_tr = ranks[train_idx], losses[train_idx]\n",
    "\n",
    "        # grid search\n",
    "        best_w, best_obj = None, np.inf\n",
    "        for w in grid:\n",
    "            obj = mean_test_loss(w, R_tr, L_tr)\n",
    "            tested_losses.append(obj)\n",
    "            if obj < best_obj:\n",
    "                best_w, best_obj = w, obj\n",
    "            elif obj == best_obj:\n",
    "                best_w_diff = ((np.full_like(best_w, 1/len(best_w)) - best_w)**2).sum()\n",
    "                w_diff = ((np.full_like(w, 1/len(w)) - w)**2).sum()\n",
    "                if w_diff < best_w_diff:\n",
    "                    best_w, best_obj = w, obj\n",
    "                \n",
    "\n",
    "        # evaluate on validation datasets of this fold\n",
    "        R_val, L_val = ranks[valid_idx], losses[valid_idx]\n",
    "        val_loss     = mean_rank_error(best_w, R_val, L_val)\n",
    "        fold_scores.append(val_loss)\n",
    "        w_sum += best_w\n",
    "\n",
    "    w_bar   = w_sum / len(cv_splits)\n",
    "    cv_mean = float(np.mean(fold_scores))\n",
    "    return w_bar, cv_mean, fold_scores\n",
    "\n",
    "w_dict = {}\n",
    "valid_scores = subsets[\"ENS\"]\n",
    "test_score = f\"class_true_{LOSS_FUNC}_full_test\"\n",
    "for approach in approach_list:\n",
    "    print(approach)\n",
    "    w_dict[approach] = {}\n",
    "    for ds in [\"spc\", \"reuters\", \"music_genres\", \"label_me\", f\"dopanim\"]:\n",
    "        print(ds)\n",
    "        w, cv_acc, fold_acc = optimise_weights_cv(\n",
    "            [v for k, v in sorted(df_dict[approach].items()) if not ds in k],\n",
    "            valid_cols=valid_scores,\n",
    "            test_col=test_score,\n",
    "            cv_splits = [(range(7, 28), range(0, 7)),\n",
    "                         (list(range(0, 7)) + list(range(14, 28)), range(7, 14)),\n",
    "                         (list(range(0, 14)) + list(range(21, 28)), range(14, 21)),\n",
    "                         (range(0, 21), range(21, 28)),\n",
    "                        ],\n",
    "            #cv_splits=[([j for j in range(28) if j != i], [i]) for i in range(28)],\n",
    "            grid_step=0.05,\n",
    "        )\n",
    "        w_dict[approach][ds] = w\n",
    "\n",
    "print(\"Optimal averaged weights :\", np.round(w, 3))\n",
    "print(\"LODO mean test accuracy  :\", cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d5d736-1f4c-471a-8e81-ba386d3c57e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.optim import RAdam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Meta2Weights(nn.Module):\n",
    "    \"\"\"\n",
    "    meta (P,)  ─►  linear  ─►  softmax  ─►  w (K,)  with Σw=1, w≥0\n",
    "    \"\"\"\n",
    "    def __init__(self, P: int, K: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(P + 1, K, bias=False)      # +1 for bias via concat‑1\n",
    "\n",
    "    def forward(self, meta: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cat([meta, torch.ones_like(meta[:, :1])], 1)  # append bias term\n",
    "        return torch.softmax(self.linear(x), 1)                 # (batch,K)\n",
    "\n",
    "def expected_test_loss(weights,    # (n,K)\n",
    "                       V_batch,    # (n,B,K)\n",
    "                       T_batch,    # (n,B)\n",
    "                       tau: float = .08) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expected test loss via a softmin with temperature τ.\n",
    "    Lower = better.  Outputs a scalar.\n",
    "    \"\"\"\n",
    "    score = (weights[:, None, :] * V_batch).sum(2)       # (n,B)\n",
    "    p     = F.softmax(-score / tau, 1)                   # prob. of picking config\n",
    "    return (p * T_batch).sum(1).mean()\n",
    "\n",
    "def learn_weights(M, V, T,\n",
    "                  splits,          # list of (train_idx, valid_idx)\n",
    "                  n_iter = 3500,\n",
    "                  lr     = 0.001,\n",
    "                  l2     = 1,\n",
    "                  ent    = 5e-3):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        w_bar      (K,)  – average of fold‑specific optima\n",
    "        cv_loss          – mean held‑out test loss\n",
    "        fold_losses      – list per fold\n",
    "    \"\"\"\n",
    "    device = \"cpu\";   # \"cuda:0\" if you have a GPU\n",
    "\n",
    "    # tensors\n",
    "    M_t = torch.as_tensor(M, dtype=torch.float32, device=device)\n",
    "    V_t = torch.as_tensor(V, dtype=torch.float32, device=device)\n",
    "    T_t = torch.as_tensor(T, dtype=torch.float32, device=device)\n",
    "\n",
    "    N, B, K = V.shape;  D = M.shape[1]\n",
    "    w_sum, fold_losses = torch.zeros(K, device=device), []\n",
    "\n",
    "    for tr, va in splits:\n",
    "        net = Meta2Weights(D, K).to(device)\n",
    "        opt = RAdam(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "        for _ in range(n_iter):\n",
    "            w_tr = net(M_t[tr])                                  # (n_tr,K)\n",
    "            loss = expected_test_loss(w_tr, V_t[tr], T_t[tr])\n",
    "\n",
    "            # entropy regulariser keeps weights spread out early on\n",
    "            ent_pen = ent * (w_tr * w_tr.clamp_min(1e-9).log()).sum(1).mean()\n",
    "\n",
    "            (loss + ent_pen).backward()\n",
    "            opt.step(); opt.zero_grad()\n",
    "\n",
    "        # ---------- validation ----------\n",
    "        with torch.no_grad():\n",
    "            w_va   = net(M_t[va])                                 # (n_va,K)\n",
    "            val_loss = expected_test_loss(w_va, V_t[va], T_t[va]).item()\n",
    "            fold_losses.append(val_loss)\n",
    "            w_sum += w_va.mean(0)\n",
    "\n",
    "    w_bar   = (w_sum / len(splits)).cpu().numpy()\n",
    "    cv_loss = float(np.mean(fold_losses))\n",
    "    return w_bar, cv_loss, fold_losses\n",
    "\n",
    "def fit_final_network(M, V, T,\n",
    "                      n_iter = 3000, lr = 1e-1,\n",
    "                      l2 = 0, ent = 1e-3, tau = 1):\n",
    "\n",
    "    N, B, K = V.shape;  D = M.shape[1]\n",
    "    dev = \"cpu\"\n",
    "\n",
    "    net = Meta2Weights(D, K).to(dev)\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "    M_t = torch.as_tensor(M, dtype=torch.float32, device=dev)\n",
    "    V_t = torch.as_tensor(V, dtype=torch.float32, device=dev)\n",
    "    T_t = torch.as_tensor(T, dtype=torch.float32, device=dev)\n",
    "    losses = []\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        w = net(M_t)                                     # (N,K)\n",
    "        loss = expected_test_loss(w, V_t, T_t, tau)\n",
    "        ent_pen = ent * (w * w.clamp_min(1e-9).log()).sum(1).mean()\n",
    "        (loss + ent_pen).backward()\n",
    "        opt.step(); opt.zero_grad()\n",
    "        losses.append(loss.cpu().detach())\n",
    "        \n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "    return net   \n",
    "\n",
    "def predict_weights(net, M_new):\n",
    "    \"\"\"\n",
    "    M_new : (n,D)  meta-features of n datasets\n",
    "    returns  (n,K)  weight vectors (rows sum to 1)\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        W = net(torch.as_tensor(M_new, dtype=torch.float32))\n",
    "    return W.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "meta_features = pd.read_csv(\"~/projects/github/multi-annotator-machine-learning/empirical_evaluation/python_scripts/metafeatures.csv\")\n",
    "splits = [(range(7, 28), range(0, 7)), (list(range(0, 7)) + list(range(14, 28)), range(7, 14)), (list(range(0, 14)) + list(range(21, 28)), range(14, 21)), (range(0, 21), range(21, 28)),]\n",
    "w_dict = {}\n",
    "valid_cols = [\n",
    "            f\"class_true_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"class_mv_unif_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"annot_unif_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"class_mv_perf_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"class_mv_perf_weights_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"annot_perf_weights_{LOSS_FUNC}_cv_valid\",\n",
    "]\n",
    "test_col = f\"class_true_{LOSS_FUNC}_full_test\"\n",
    "for approach in approach_list:\n",
    "    print(approach)\n",
    "    w_dict[approach] = {}\n",
    "    for ds in [\"spc\", \"reuters\", \"music_genres\", \"label_me\", f\"dopanim\"]:\n",
    "        print(ds)\n",
    "        dfs = [v for k, v in sorted(df_dict[approach].items())]\n",
    "        is_not_ds = np.array([ds not in ds_name for ds_name in meta_features[\"dataset\"].values])\n",
    "        M = meta_features.values[:, 1:].astype(float)\n",
    "        sc = StandardScaler().fit(M)\n",
    "        M = sc.transform(M)\n",
    "        N, K = len(dfs), len(valid_cols)\n",
    "        L, T = [], []\n",
    "        for df in dfs:\n",
    "            L.append(df[valid_cols])                       # (B, K)\n",
    "            T.append(df[test_col].to_numpy())  # (B,)\n",
    "        V  = np.stack(L).astype(float)   # (N, B, K)\n",
    "        T = np.stack(T).astype(float)  # (N, B)\n",
    "\n",
    "        net_final = fit_final_network(V=torch.from_numpy(V).float(), T=torch.from_numpy(T).float(), M=torch.from_numpy(M).float())\n",
    "        M_new = meta_features[~is_not_ds].values[:, 1:].astype(float)\n",
    "        M_new = sc.transform(M_new)\n",
    "        W_new = predict_weights(net_final, M_new)\n",
    "        for i, ds_new in enumerate(meta_features[~is_not_ds][\"dataset\"]):\n",
    "            w_dict[approach][ds_new] = W_new[i]\n",
    "        print(W_new.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5333d0-2178-48b4-9849-80c3e001afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.optim import RAdam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from lightgbm import early_stopping, log_evaluation, LGBMClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "meta_features = pd.read_csv(\"~/projects/github/multi-annotator-machine-learning/empirical_evaluation/python_scripts/metafeatures.csv\")\n",
    "test_losses = {}\n",
    "valid_cols = [\n",
    "            #f\"class_true_{LOSS_FUNC}_full_test\",\n",
    "            f\"class_smv_unif_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"annot_unif_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"class_smv_perf_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"class_smv_perf_weights_{LOSS_FUNC}_cv_valid\",\n",
    "            f\"annot_perf_weights_{LOSS_FUNC}_cv_valid\",\n",
    "            'gt_lr', 'gt_wd', 'ap_lr', 'ap_wd', 'bs', 'dr', \n",
    "            'lmbda', 'eta', 'alpha', 'beta', 'dim', 'epsilon',\n",
    "]\n",
    "test_col = f\"class_true_{LOSS_FUNC}_full_test\"\n",
    "for approach in approach_list:\n",
    "    print(approach)\n",
    "    test_losses[approach] = {}\n",
    "    for ds in [\"music_genres\", \"label_me\", \"dopanim\", \"reuters\", \"spc\"]:\n",
    "        \n",
    "        # Training data.\n",
    "        dfs_train = [v for k, v in sorted(df_dict[approach].items())  if not ds in k]\n",
    "        is_train = np.array([ds not in ds_name for ds_name in meta_features[\"dataset\"].values])\n",
    "        M_train = meta_features[is_train].values[:, 1:-1].astype(float)\n",
    "        V_train, T_train = [], []\n",
    "        for df in dfs_train:\n",
    "            V_train.append(df[valid_cols])                       \n",
    "            T_train.append(df[test_col])\n",
    "        V_train  = np.stack(V_train)\n",
    "        is_nan_or_non = np.logical_or(np.isnan(V_train.astype(object)).any(axis=0), (V_train == None).any(axis=0))\n",
    "        V_train = V_train[~is_nan_or_non]\n",
    "        T_train = np.stack(T_train).astype(float)  \n",
    "        N_train, B_train, K_train = V_train.shape\n",
    "        X_train = np.hstack([\n",
    "            -V_train.reshape(N_train * B_train, K_train),          \n",
    "            np.repeat(M_train, B_train, axis=0)        \n",
    "        ])\n",
    "        X_train[np.isnan(X_train)] = 0\n",
    "        sc = StandardScaler().fit(X_train)\n",
    "        X_train = sc.transform(X_train)\n",
    "        ranks_train  = np.argsort(T_train, axis=1)\n",
    "        grades_train = (B_train - 1) - ranks_train                             \n",
    "        q_train      = np.quantile(grades_train, np.arange(0.1, 1.0, 0.05))\n",
    "        y_train  = np.digitize(grades_train, q).reshape(-1)   \n",
    "        group_sizes_full_train = np.repeat(B_train, N_train)                   \n",
    "        query_ids_train = np.repeat(np.arange(N_train), B_train)\n",
    "        dtrain = lgb.Dataset(X_train, label=y_train, group=group_sizes_full_train)\n",
    "\n",
    "        \n",
    "        # Test data.\n",
    "        dfs_test = [v for k, v in sorted(df_dict[approach].items()) if ds in k]\n",
    "        test_sets = [k for k, v in sorted(df_dict[approach].items()) if ds in k]\n",
    "        is_test = np.array([ds in ds_name for ds_name in meta_features[\"dataset\"].values])\n",
    "        M_test = meta_features[is_test].values[:, 1:-1].astype(float)\n",
    "        V_test, T_test = [], []\n",
    "        for df in dfs_test:\n",
    "            V_test.append(df[valid_cols])                       \n",
    "            T_test.append(df[test_col])\n",
    "        V_test  = np.stack(V_test).astype(float)  \n",
    "        T_test = np.stack(T_test).astype(float)\n",
    "        N_test, B_test, K_test = V_test.shape\n",
    "        X_test = np.hstack([\n",
    "            -V_test.reshape(N_test * B_test, K_test),          \n",
    "            np.repeat(M_test, B_test, axis=0)        \n",
    "        ])\n",
    "        X_test[np.isnan(X_test)] = 0\n",
    "        X_test = sc.transform(X_test)\n",
    "        ranks_test = np.argsort(T_test, axis=1)\n",
    "        grades_test = (B_test - 1) - ranks_test                            \n",
    "        q_test = np.quantile(grades_test, np.arange(0.1, 1.0, 0.05))\n",
    "        y_test = np.digitize(grades_test, q).reshape(-1) \n",
    "        group_sizes_full_test = np.repeat(B_test, N_test)                   \n",
    "        query_ids_test = np.repeat(np.arange(N_test), B_test) \n",
    "        \n",
    "        \n",
    "        # Train model.\n",
    "        params = {\n",
    "            \"objective\": \"lambdarank\",   \n",
    "            \"metric\": \"ndcg\",                 \n",
    "            \"learning_rate\": 0.001,\n",
    "            \"num_leaves\": 31,\n",
    "            \"min_data_in_leaf\": 1,\n",
    "            \"verbose\": -1                     \n",
    "        }\n",
    "        model = lgb.train(params, dtrain, num_boost_round=10)\n",
    "        #lgb.plot_importance(model, importance_type=\"gain\", figsize=(7,6), title=\"LightGBM Feature Importance (Gain)\")\n",
    "        #plt.show()\n",
    "        reg = LinearRegression().fit(X_train, T_train.reshape(-1))\n",
    "        \n",
    "        # Make predictions.\n",
    "        for g in np.unique(query_ids_test):\n",
    "            is_g = g == query_ids_test\n",
    "            preds_d = -model.predict(X_test[is_g])\n",
    "            preds_d = reg.predict(X_test[is_g])\n",
    "            print(f\"{test_sets[g]}: {T_test.reshape(-1)[is_g][preds_d.argmin()]}\")\n",
    "            test_losses[approach][test_sets[g]] = T_test.reshape(-1)[is_g][preds_d.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1520341-42f5-4929-a61b-5aca9c0ad0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cdd4da-377b-4b01-bd46-b0d89dcc339b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dict[\"dawid-skene\"][\"spc_worst-1\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75dd4c-961c-47c3-afce-0df15948a7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
